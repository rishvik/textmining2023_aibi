{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4fqP9NQyXhg"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/MeMartijn/updated-sklearn-crfsuite.git#egg=sklearn_crfsuite\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "#stop_words = set(stopwords.words('english'))\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "from itertools import chain\n",
        "import sklearn\n",
        "import scipy.stats\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
        "\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
      ],
      "metadata": {
        "id": "sOvmxpHGxofh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhxTZqZuwKwr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prvWX5nNORf4"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ndmr_Jagv6wi"
      },
      "outputs": [],
      "source": [
        "def get_root(phrase):\n",
        "  for token in phrase:\n",
        "    if token.dep_ == 'ROOT':\n",
        "      return token\n",
        "\n",
        "#takes a phrase as input in the tokenized fromat\n",
        "#breadth-first search of the dependency tree of the given phrase \n",
        "#returns a the dictionary with the length of the path to root for each token in the sentence     \n",
        "def len_path_root(phrase):\n",
        "  dist = 0\n",
        "  root_token = get_root(phrase)\n",
        "  children = list(root_token.children)\n",
        "  lengths = {\n",
        "      root_token: dist\n",
        "  }\n",
        "  children_count = len(list(children))\n",
        "  while children_count != 0:\n",
        "    dist += 1\n",
        "    new_children = []\n",
        "    for token in children:\n",
        "      lengths[token] = dist\n",
        "      new_children.extend(token.children)\n",
        "    children=new_children\n",
        "    children_count = len(list(new_children))\n",
        "  \n",
        "  #assign length -1 (chosen randomly) for nodes not connected to the root \n",
        "  for token in phrase:\n",
        "    if token not in lengths:\n",
        "      lengths[token] = -1\n",
        "  \n",
        "  return lengths\n",
        "\n",
        "#takes the dataset, phrase_id and chapter_id\n",
        "#retruns the phrase in a string by merging the words\n",
        "#used to iterate in the training and testing set for extracting phrases \n",
        "def get_text(table, id,chapter):\n",
        "  phrase_table = table[(table['phrase_id'] == id) & (table['chapter_id'] == chapter)]\n",
        "  return ' '.join(join_punctuation(phrase_table['word'].values))\n",
        "\n",
        "def join_punctuation(seq, characters='.,;?!'):\n",
        "    characters = set(characters)\n",
        "    seq = iter(seq)\n",
        "    current = next(seq)\n",
        "\n",
        "    for nxt in seq:\n",
        "        if nxt in characters:\n",
        "            current += nxt\n",
        "        else:\n",
        "            yield current\n",
        "            current = nxt\n",
        "\n",
        "    yield current\n",
        "\n",
        "# def get_text(table, id,chapter):\n",
        "#   phrase_table = table[(table['phrase_id'] == id) & (table['chapter_id'] == chapter)]\n",
        "#   return ' '.join(phrase_table['word'].values)\n",
        "\n",
        "#returns a list with the labels of a phrase identified with the ch_id, and phr_id\n",
        "def get_labels(table, id,chapter):\n",
        "  phrase_table = table[(table['phrase_id'] == id) & (table['chapter_id'] == chapter)]\n",
        "  return phrase_table['label'].values\n",
        "\n",
        "#adjusted lemmatization for nltk library\n",
        "#offers the POS as a parameter to lemmatization function to make it more precise\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "#used to match the length of the tokenization with the length of the filtered table \n",
        "#initially implemented for the previous assignment of data preprocessing\n",
        "#treats separately some excpetions found\n",
        "def tokenize(arg, ch = 'baskervilles03', ph = 21):\n",
        "\n",
        "  if (ph, ch) == (436, 'wisteria02'):\n",
        "    with arg.retokenize() as retokenizer:\n",
        "      retokenizer.merge(arg[0:2])\n",
        "      retokenizer.merge(arg[4:7])\n",
        "    return arg\n",
        "\n",
        "  if (ph, ch) in [(450, 'cardboard'),(457, 'cardboard')]:\n",
        "    with arg.retokenize() as retokenizer:\n",
        "      retokenizer.merge(arg[2:4])\n",
        "      retokenizer.merge(arg[0:2])\n",
        "    return arg\n",
        "\n",
        "\n",
        "  no_exc = [('baskervilles03', 16), ('baskervilles03', 20), ('baskervilles11', 45), ('baskervilles12', 283), ('baskervilles13', 271), ('baskervilles14', 55)]\n",
        "  retok1_pos = []#for - \n",
        "  retok2_pos = []#for `\n",
        "\n",
        "  #1\n",
        "  shift = 0\n",
        "  cr_pos = 0\n",
        "  for token in arg:\n",
        "    if token.text == '-':\n",
        "      retok1_pos.append(cr_pos)\n",
        "    cr_pos+=1\n",
        "    prev_char = token.text\n",
        "  \n",
        "  for pos in retok1_pos:\n",
        "    with arg.retokenize() as retokenizer:\n",
        "      retokenizer.merge(arg[pos-1-shift:pos+2-shift])\n",
        "      shift += 2 \n",
        "\n",
        "  #2\n",
        "  shift = 0\n",
        "  cr_pos = 0\n",
        "  prev_char = 0\n",
        "  for token in arg:\n",
        "    if token.text =='`' and prev_char == '`':\n",
        "      retok2_pos.append(cr_pos)\n",
        "    cr_pos+=1\n",
        "    prev_char = token.text\n",
        " \n",
        "  for pos in retok2_pos:\n",
        "    with arg.retokenize() as retokenizer:\n",
        "      retokenizer.merge(arg[pos-shift-1:pos+1-shift])\n",
        "      shift += 1\n",
        "\n",
        "  #3\n",
        "  retok2_pos = []\n",
        "  suf = ['66', '86','ve','m']\n",
        "  shift = 0\n",
        "  cr_pos = 0\n",
        "  prev_char = 0\n",
        "  for token in arg:\n",
        "    if token.text in suf and prev_char == \"'\" or token.text == '.' and prev_char == \"No\" and (ch,ph) not in no_exc:\n",
        "      retok2_pos.append(cr_pos)\n",
        "    cr_pos+=1\n",
        "    prev_char = token.text\n",
        " \n",
        "  for pos in retok2_pos:\n",
        "    with arg.retokenize() as retokenizer:\n",
        "      retokenizer.merge(arg[pos-shift-1:pos+1-shift])\n",
        "      shift += 1\n",
        "\n",
        "  if ph in [0,'0']:\n",
        "    with arg.retokenize() as retokenizer:\n",
        "      retokenizer.merge(arg[2:4])\n",
        "\n",
        "  \n",
        "  return arg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyc_QQ8XSKgt"
      },
      "source": [
        "## [Not needed anymore] Testing attributes on individual pre-set phrase before automatically adding to the table dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "GJAAaSh264RX",
        "outputId": "6c074eb9-6af8-42cf-f3ee-abd938674169"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-320a7629260e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#testing if tokenize and data from table have the same length for each phrase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mall_ch_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chapter_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmismatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_ch_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mfilter_ch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chapter_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ],
      "source": [
        "#testing if tokenize and data from table have the same length for each phrase\n",
        "all_ch_ids = train_data['chapter_id'].unique()\n",
        "mismatch=[]\n",
        "for ch in all_ch_ids:\n",
        "  filter_ch = train_data[train_data['chapter_id'] == ch]\n",
        "  all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "  for ph in all_ph_ids:\n",
        "    filter_ph = filter_ch[filter_ch['phrase_id'] == ph]\n",
        "    phrase = get_text(ph, ch)\n",
        "    phr_doc = nlp(phrase)\n",
        "    tok = tokenize(phr_doc, ch, ph)\n",
        "    if len(filter_ph) != len(tok):\n",
        "      mismatch.append((ch,ph))\n",
        "print(len(mismatch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QhrBL7XXqgm"
      },
      "outputs": [],
      "source": [
        "#testing if there is any negation in \n",
        "all_ch_ids = train_data['chapter_id'].unique()\n",
        "mismatch=[]\n",
        "for ch in all_ch_ids:\n",
        "  filter_ch = train_data[train_data['chapter_id'] == ch]\n",
        "  all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "  for ph in all_ph_ids:\n",
        "    filter_ph = filter_ch[filter_ch['phrase_id'] == ph]\n",
        "    phrase = get_text(ph, ch)\n",
        "    phr_doc = nlp(phrase)\n",
        "    tok = tokenize(phr_doc, ch, ph)\n",
        "    if len(filter_ph) != len(tok):\n",
        "      mismatch.append((ch,ph))\n",
        "print(len(mismatch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4jSB2po4otL"
      },
      "outputs": [],
      "source": [
        "#errors\n",
        "phrase = get_text(436, 'wisteria02')\n",
        "phr_doc = nlp(phrase)\n",
        "#toks = tokenize(phr_doc,436)\n",
        "for tok in phr_doc:\n",
        "  print(tok,'\\n')\n",
        "\n",
        "# flag = 0\n",
        "# if(all(x in abc for x in mismatch)):\n",
        "#     flag = 1\n",
        "# print(flag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_moUSU35c59d"
      },
      "outputs": [],
      "source": [
        "#nbor(d) - neighbour in the initial sentence at distance d -/+ -> to left/right\n",
        "phr1 = \"He is interested in learning Natural Language Processing.\"\n",
        "phr2 = \"I stood upon the hearth-rug and picked up the stick which our visitor had left behind him the night before.\"\n",
        "phr3 = \"Gus Proto is a Python developer currently working for a London-based Fintech company\"\n",
        "\n",
        "phr_doc = nlp(phr2)\n",
        "res = len_path_root(phr_doc)\n",
        "\n",
        "for token in phr_doc:\n",
        "  print(token.text, res[token], \"\\n\")\n",
        "\n",
        "displacy.render(phr_doc, style=\"dep\", jupyter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJdzOnF1AGVY"
      },
      "outputs": [],
      "source": [
        "phr4 = \"Mr. Sherlock Holmes , who was usually very late in the mornings , save upon those not infrequent occasions when he was up all night , was seated at the breakfast table .\"\n",
        "phr4 = \"guru99 is a totally new kind of learning experience.\"\n",
        "phr4 = \"The striped bats are hanging on their feet for best\"\n",
        "\n",
        "#phr4 = sent_tokenize(phr4)\n",
        "words_list = nltk.word_tokenize(phr4)\n",
        "print(tokenize(words_list))\n",
        "print(words_list)\n",
        "#adjusted lemma\n",
        "#lemmatizer = WordNetLemmatizer()\n",
        "#print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words_list])\n",
        "\n",
        "#POS 1,2\n",
        "# fine_tags = nltk.pos_tag(words_list)\n",
        "# coarse_tags = nltk.pos_tag(words_list, tagset='universal')\n",
        "# print(fine_tags)\n",
        "# print(coarse_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue9yXRR9-HuZ"
      },
      "outputs": [],
      "source": [
        "id = 15\n",
        "ch = 'baskervilles01'\n",
        "txt = get_text(id,ch)\n",
        "txt = \"I wouldn't do that\"\n",
        "phr_doc = nlp(txt)\n",
        "\n",
        "#print(len(list(phr_doc)))\n",
        "\n",
        "# with phr_doc.retokenize() as retokenizer:\n",
        "#     retokenizer.merge(phr_doc[21:23])\n",
        "\n",
        "#phr_doc = re_tokenize(phr_doc)\n",
        "for token in phr_doc:\n",
        "  print(token)\n",
        "\n",
        "#displacy.render(phr_doc, style=\"dep\", jupyter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLCw19tK1Cvy"
      },
      "outputs": [],
      "source": [
        "#backup\n",
        "def sent2feature(sentence, ch = 'baskervilles03', ph = 21):\n",
        "  sent_feat = []\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  sent_doc = nlp(sentence)\n",
        "  tokens = tokenize(sent_doc, ch, ph)\n",
        "  lengths = len_path_root(tokens)\n",
        "\n",
        "  ord = 0\n",
        "  shift = 0\n",
        "  for tok in tokens:\n",
        "    features = word2feature(tok)\n",
        "    features['len_path_root'] = lengths[tok]\n",
        "    sent_feat.append(features)\n",
        "\n",
        "  return sent_feat\n",
        "\n",
        "def process_data(table):\n",
        "  all_ch_ids = table['chapter_id'].unique()\n",
        "  all_features = []\n",
        "  all_labels = []\n",
        "  for ch in all_ch_ids:\n",
        "    filter_ch = table[table['chapter_id'] == ch]\n",
        "    all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "    for ph in all_ph_ids:\n",
        "      filter_ph = filter_ch[filter_ch['phrase_id'] == ph]\n",
        "      phrase = get_text(table, ph, ch)\n",
        "      labels = get_labels(table, ph, ch)\n",
        "      all_features.append(sent2feature(phrase, ch, ph))\n",
        "      all_labels.append(labels)\n",
        "\n",
        "  return all_features, all_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7d-MsmDOeCX"
      },
      "source": [
        "## Importing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bViK7iTB7jLq"
      },
      "outputs": [],
      "source": [
        "#merging test datasets\n",
        "test_card = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/ATM/SEM-2012-test-cardboard.txt', sep=\"\\t\", header = None)\n",
        "test_circ = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/ATM/SEM-2012-test-circle.txt', sep=\"\\t\", header = None)\n",
        "\n",
        "frames = [test_card, test_circ]\n",
        "test_data = pd.concat(frames)\n",
        "test_data.rename(columns={1: 'phrase_id', 0: 'chapter_id', 2:'word_id', 3:'word', 4:'label'}, inplace=True)\n",
        "#print(test_data.head(40))\n",
        "\n",
        "#train & dev\n",
        "train_data = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/ATM/SEM-2012-training.txt', sep=\"\\t\", header = None)\n",
        "dev = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/ATM/SEM-2012-dev.txt', sep=\"\\t\", header = None)\n",
        "\n",
        "train_data.rename(columns={1: 'phrase_id', 0: 'chapter_id', 2:'word_id', 3:'word', 4:'label'}, inplace=True)\n",
        "dev.rename(columns={1: 'phrase_id', 0: 'chapter_id', 2:'word_id', 3:'word', 4:'label'}, inplace=True)\n",
        "\n",
        "#merged datasets in test_data; train_data and dev separate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(dev.head(10))\n",
        "#counting B-Neg values\n",
        "print(train_data['label'].value_counts()['B-NEG'])"
      ],
      "metadata": {
        "id": "1ghqP6SbIppy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcQPgzSOkyCZ"
      },
      "source": [
        "## Dataset exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pch_WI7txN1z",
        "outputId": "7aaca6f2-6380-4fae-f092-b75b4b14b4be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['baskervilles01' 'baskervilles02' 'baskervilles03' 'baskervilles04'\n",
            " 'baskervilles05' 'baskervilles06' 'baskervilles07' 'baskervilles08'\n",
            " 'baskervilles09' 'baskervilles10' 'baskervilles11' 'baskervilles12'\n",
            " 'baskervilles13' 'baskervilles14' 'wisteria01' 'wisteria02']\n"
          ]
        }
      ],
      "source": [
        "all_ch_ids = train_data['chapter_id'].unique()\n",
        "print(all_ch_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr-rHortyOJJ"
      },
      "outputs": [],
      "source": [
        "#number of phrases/chapter\n",
        "all_ch_ids = train_data['chapter_id'].unique()\n",
        "all_features = []\n",
        "all_labels = []\n",
        "for ch in all_ch_ids:\n",
        "  filter_ch = train_data[train_data['chapter_id'] == ch]\n",
        "  #print(len(filter_ch))\n",
        "  all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "  print(len(all_ph_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akPDVdZlkv82",
        "outputId": "f2c11bb0-e6ef-433c-f01f-3a1e203dc850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83\n",
            "2\n",
            "17.961306256860592\n"
          ]
        }
      ],
      "source": [
        "#num chapters training\n",
        "phrase_lengths = []\n",
        "\n",
        "all_ch_ids = train_data['chapter_id'].unique()\n",
        "all_features = []\n",
        "all_labels = []\n",
        "for ch in all_ch_ids:\n",
        "  filter_ch = train_data[train_data['chapter_id'] == ch]\n",
        "  all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "  for ph in all_ph_ids:\n",
        "    filter_ph = filter_ch[filter_ch['phrase_id'] == ph]\n",
        "    phrase_lengths.append(len(filter_ph))\n",
        "\n",
        "print(max(phrase_lengths))\n",
        "print(min(phrase_lengths))\n",
        "print(sum(phrase_lengths) / len(phrase_lengths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G5i7Uo91Gj7",
        "outputId": "37218621-bce0-4383-d902-2a24d9f340cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65451\n",
            "13567\n"
          ]
        }
      ],
      "source": [
        "print(len(train_data))\n",
        "print(len(dev))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4ERpOk813Fz",
        "outputId": "8b99d712-4728-4811-9fc0-cb03af38a2f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['cardboard' 'circle01' 'circle02']\n"
          ]
        }
      ],
      "source": [
        "all_ch_ids = test_data['chapter_id'].unique()\n",
        "print(all_ch_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PV2IjaVb14yp",
        "outputId": "f32bdefd-6a63-4176-870e-b651b31780f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "496\n",
            "371\n",
            "222\n"
          ]
        }
      ],
      "source": [
        "all_ch_ids = test_data['chapter_id'].unique()\n",
        "all_features = []\n",
        "all_labels = []\n",
        "for ch in all_ch_ids:\n",
        "  filter_ch = test_data[test_data['chapter_id'] == ch]\n",
        "  #print(len(filter_ch))\n",
        "  all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "  print(len(all_ph_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRtADJDu19X-",
        "outputId": "c8889f8d-a789-49e5-f7c9-3ed5da4a30df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "68\n",
            "2\n",
            "17.6455463728191\n"
          ]
        }
      ],
      "source": [
        "phrase_lengths = []\n",
        "\n",
        "all_ch_ids = test_data['chapter_id'].unique()\n",
        "all_features = []\n",
        "all_labels = []\n",
        "for ch in all_ch_ids:\n",
        "  filter_ch = test_data[test_data['chapter_id'] == ch]\n",
        "  all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "  for ph in all_ph_ids:\n",
        "    filter_ph = filter_ch[filter_ch['phrase_id'] == ph]\n",
        "    phrase_lengths.append(len(filter_ph))\n",
        "\n",
        "print(max(phrase_lengths))\n",
        "print(min(phrase_lengths))\n",
        "print(sum(phrase_lengths) / len(phrase_lengths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQWknNlg6j6r"
      },
      "outputs": [],
      "source": [
        "print(list(train_data['label'].values).count('B-NEG'))\n",
        "print(list(train_data['label'].values).count('I-NEG'))\n",
        "print(list(train_data['label'].values).count('O'))\n",
        "\n",
        "print('\\n', \"dev\")\n",
        "print(list(dev['label'].values).count('B-NEG'))\n",
        "print(list(dev['label'].values).count('I-NEG'))\n",
        "print(list(dev['label'].values).count('O'))\n",
        "\n",
        "print('\\n', \"test\")\n",
        "print(list(test_data['label'].values).count('B-NEG'))\n",
        "print(list(test_data['label'].values).count('I-NEG'))\n",
        "print(list(test_data['label'].values).count('O'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR5-pL5miPhX"
      },
      "source": [
        "## CRF functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fA7sKYLLagzN"
      },
      "outputs": [],
      "source": [
        "#takes a token as input\n",
        "#returns True if token should be kept, or False if it is filtered\n",
        "#could be changed depending on performance\n",
        "def keep(tok):\n",
        "  neg_list = ['nor', 'Nor', 'neither', 'Neither', 'without', 'Without', 'nobody', 'Nobody', 'none', 'None', 'nothing', 'Nothing', \n",
        "            'never', 'not', 'no', 'Never', 'Not', 'No', 'nowhere', 'non', 'Nowhere', 'Non', \"n't\", \"rather\", \"than\", 'for', 'the']\n",
        "  if tok.text in neg_list:\n",
        "    return True\n",
        "  if tok.is_punct or tok.is_stop or tok.text == \"``\":\n",
        "    return False\n",
        "  return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZyP6il2CFNvS"
      },
      "outputs": [],
      "source": [
        "def word2feature(token):\n",
        "  prefixes = ['un', 'in', 'im','il', 'dis', 'non', 'ir',\n",
        "              'Un', 'In', 'Im','Il', 'Dis', 'Non', 'Ir']\n",
        "  \n",
        "  pref = 0\n",
        "  for p in prefixes:\n",
        "    if token.text.startswith(p):\n",
        "      pref = 1\n",
        "      break\n",
        "  \n",
        "  suf = 0\n",
        "  if 'less' in token.text:\n",
        "    suf = 1\n",
        "   \n",
        "  features = {\n",
        "    'text': token.text,\n",
        "    'lemma':token.lemma_,\n",
        "    'fine_pos': token.pos_,\n",
        "    'coarse_pos': token.tag_,\n",
        "    'dependency':token.dep_,\n",
        "    'head':token.head.text,\n",
        "    'suffix':suf,\n",
        "    'prefix': pref\n",
        "  }\n",
        "\n",
        "  return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lXnUKNHQ-fAE"
      },
      "outputs": [],
      "source": [
        "#use this\n",
        "\n",
        "#takes as input text of a sentence\n",
        "#returns a list of dictionaries with the features of its tokens\n",
        "def sent2feature(sentence, labels, is_test, ch = 'baskervilles03', ph = 21):\n",
        "  sent_feat = []\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  sent_doc = nlp(sentence)\n",
        "  tokens = tokenize(sent_doc, ch, ph)\n",
        "  lengths = len_path_root(tokens)\n",
        "\n",
        "  ord = 0\n",
        "  shift = 0\n",
        "  for tok in tokens:\n",
        "\n",
        "    if keep(tok) or is_test:\n",
        "      features = word2feature(tok)\n",
        "      features['len_path_root'] = lengths[tok]\n",
        "      sent_feat.append(features)\n",
        "\n",
        "    else:\n",
        "      labels = np.delete(labels, ord-shift)\n",
        "      shift+=1\n",
        "\n",
        "    ord+=1\n",
        "\n",
        "  return sent_feat, labels\n",
        "\n",
        "#takes the table as an input\n",
        "#is_test makes the preporcessing function keep all entries in case of test data \n",
        "#returns the list of lists of dicitionaries with the features\n",
        "#text->phrase->words \n",
        "def process_data(table, is_test):\n",
        "  all_ch_ids = table['chapter_id'].unique()\n",
        "  all_features = []\n",
        "  all_labels = []\n",
        "  for ch in all_ch_ids:\n",
        "    filter_ch = table[table['chapter_id'] == ch]\n",
        "    all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "    for ph in all_ph_ids:\n",
        "      phrase = get_text(table, ph, ch)\n",
        "      labels = get_labels(table, ph, ch)\n",
        "      #filtered\n",
        "      #print(ch,ph)\n",
        "      filt_features, filt_labels = sent2feature(phrase, labels, is_test, ch, ph)\n",
        "      all_features.append(filt_features)\n",
        "      all_labels.append(filt_labels)\n",
        "\n",
        "  return all_features, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#solving exceptions in data\n",
        "def filter(x,y):\n",
        "  new_y = []\n",
        "  for sent,lab in zip(x, y):\n",
        "    if len(sent) != len(lab):\n",
        "      lab = np.delete(lab, len(lab)-1)\n",
        "    \n",
        "    #tried replacing B-Neg tag with B-LOC, as this belongs to crf.classes_\n",
        "    # repl = []\n",
        "    # for l in lab:\n",
        "    #   if l == 'B-NEG':\n",
        "    #     repl.append('B-LOC')\n",
        "    #   elif l == 'I-NEG':\n",
        "    #     repl.append('I-LOC')\n",
        "    #   else:\n",
        "    #     repl.append(l)\n",
        "    #   lab = repl\n",
        "    #\n",
        "\n",
        "    new_y.append(lab)\n",
        "\n",
        "  return new_y"
      ],
      "metadata": {
        "id": "JHU3ESNOjcPy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0Y-kxBedey9V"
      },
      "outputs": [],
      "source": [
        "#data final form\n",
        "x_train, y_trainin = process_data(train_data, False)\n",
        "x_test, y_test = process_data(dev, True)\n",
        "\n",
        "y_train = filter(x_train,y_trainin)\n",
        "y_test = filter(x_test,y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing code"
      ],
      "metadata": {
        "id": "JEX7l7tQTeYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train2[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFoTgU82rvD1",
        "outputId": "edd07df0-d1ae-469e-d776-025e5c526ca3"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train), len(y_train))\n",
        "print(len(x_test), len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekJAulp2UkmA",
        "outputId": "7dc61045-3143-4c50-8be5-f22dee945fc4"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3644 3644\n",
            "787 787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # for word in sent:\n",
        "  #   for feat in word:\n",
        "  #     if :\n",
        "  #       print(word, feat)\n",
        "  #       break"
      ],
      "metadata": {
        "id": "KOuWzYSCPbMy"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JK2JFA2nUKmu"
      },
      "outputs": [],
      "source": [
        "#counting B-Neg from dataset\n",
        "count = 0\n",
        "for sent in y_train:\n",
        "  for tok in sent:\n",
        "    if tok == 'B-NEG':\n",
        "      count+=1\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#iterate through tokens of sentence\n",
        "doc = nlp(\"I don't like apples and pasta.\")\n",
        "for tok in doc:\n",
        "  print(isinstance(tok, spacy.tokens.token.Token))"
      ],
      "metadata": {
        "id": "VJF7g4baGBEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIqDLgqrVXS3"
      },
      "outputs": [],
      "source": [
        "#extracting features for a single sentence\n",
        "phr1 = get_text(train_data, 121, 'baskervilles08')\n",
        "lab1 = get_labels(train_data, 121, 'baskervilles08')\n",
        "doc = nlp(phr1)\n",
        "doc = tokenize(doc)\n",
        "\n",
        "#printing tokens\n",
        "for tok in doc:\n",
        "  print(tok.text, tok.pos_)\n",
        "\n",
        "#printing extracted features\n",
        "ld = sent2feature(phr1, lab1, 'baskervilles08', 121)\n",
        "for word_dict in ld[0]:\n",
        "  print(word_dict, '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CRF tutorial with original code"
      ],
      "metadata": {
        "id": "_qYZR8KdAIKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn-crfsuite"
      ],
      "metadata": {
        "id": "dI57q21Jy8vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'postag': postag,\n",
        "        'postag[:2]': postag[:2],\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            '-1:postag': postag1,\n",
        "            '-1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:postag': postag1,\n",
        "            '+1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, postag, label in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, postag, label in sent]"
      ],
      "metadata": {
        "id": "crhX1ImazeoP"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('conll2002')\n",
        "nltk.corpus.conll2002.fileids()\n",
        "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
        "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))\n",
        "X_train = [sent2features(s) for s in train_sents]\n",
        "Y_train = [sent2labels(s) for s in train_sents]\n",
        "\n",
        "X_test = [sent2features(s) for s in test_sents]\n",
        "Y_test = [sent2labels(s) for s in test_sents]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4wFLUZVzUeG",
        "outputId": "809847ff-f45e-42fc-ad28-15351a8dbc80"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2002 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y_train[7])"
      ],
      "metadata": {
        "id": "oa-7TOjF4z2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[7])"
      ],
      "metadata": {
        "id": "Ia613-BS5ILI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(X_train[7])\n",
        "for feat in X_train[7]:\n",
        "  print(feat)"
      ],
      "metadata": {
        "id": "IFs2eTVl0LwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for feat in x_train[1]:\n",
        "  print(feat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b1TOR7j2QBr",
        "outputId": "2201be57-7944-407c-e2b4-0df0f736c5b2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bias': 1.0, 'text': 'Mr.', 'lemma': 'Mr.', 'fine_pos': 'PROPN', 'coarse_pos': 'NNP', 'dependency': 'compound', 'head': Holmes, 'suffix': 0, 'prefix': 0, 'len_path_root': 2}\n",
            "{'bias': 1.0, 'text': 'Sherlock', 'lemma': 'Sherlock', 'fine_pos': 'PROPN', 'coarse_pos': 'NNP', 'dependency': 'compound', 'head': Holmes, 'suffix': 0, 'prefix': 0, 'len_path_root': 2}\n",
            "{'bias': 1.0, 'text': 'Holmes', 'lemma': 'Holmes', 'fine_pos': 'PROPN', 'coarse_pos': 'NNP', 'dependency': 'nsubj', 'head': save, 'suffix': 0, 'prefix': 0, 'len_path_root': 1}\n",
            "{'bias': 1.0, 'text': 'usually', 'lemma': 'usually', 'fine_pos': 'ADV', 'coarse_pos': 'RB', 'dependency': 'advmod', 'head': was, 'suffix': 0, 'prefix': 0, 'len_path_root': 3}\n",
            "{'bias': 1.0, 'text': 'late', 'lemma': 'late', 'fine_pos': 'ADV', 'coarse_pos': 'RB', 'dependency': 'acomp', 'head': was, 'suffix': 0, 'prefix': 0, 'len_path_root': 3}\n",
            "{'bias': 1.0, 'text': 'the', 'lemma': 'the', 'fine_pos': 'DET', 'coarse_pos': 'DT', 'dependency': 'det', 'head': mornings, 'suffix': 0, 'prefix': 0, 'len_path_root': 5}\n",
            "{'bias': 1.0, 'text': 'mornings', 'lemma': 'morning', 'fine_pos': 'NOUN', 'coarse_pos': 'NNS', 'dependency': 'pobj', 'head': in, 'suffix': 0, 'prefix': 0, 'len_path_root': 4}\n",
            "{'bias': 1.0, 'text': 'save', 'lemma': 'save', 'fine_pos': 'VERB', 'coarse_pos': 'VB', 'dependency': 'ROOT', 'head': save, 'suffix': 0, 'prefix': 0, 'len_path_root': 0}\n",
            "{'bias': 1.0, 'text': 'not', 'lemma': 'not', 'fine_pos': 'PART', 'coarse_pos': 'RB', 'dependency': 'neg', 'head': occasions, 'suffix': 0, 'prefix': 0, 'len_path_root': 3}\n",
            "{'bias': 1.0, 'text': 'infrequent', 'lemma': 'infrequent', 'fine_pos': 'ADJ', 'coarse_pos': 'JJ', 'dependency': 'amod', 'head': occasions, 'suffix': 0, 'prefix': 1, 'len_path_root': 3}\n",
            "{'bias': 1.0, 'text': 'occasions', 'lemma': 'occasion', 'fine_pos': 'NOUN', 'coarse_pos': 'NNS', 'dependency': 'pobj', 'head': upon, 'suffix': 0, 'prefix': 0, 'len_path_root': 2}\n",
            "{'bias': 1.0, 'text': 'night', 'lemma': 'night', 'fine_pos': 'NOUN', 'coarse_pos': 'NN', 'dependency': 'npadvmod', 'head': was, 'suffix': 0, 'prefix': 0, 'len_path_root': 2}\n",
            "{'bias': 1.0, 'text': 'seated', 'lemma': 'seat', 'fine_pos': 'VERB', 'coarse_pos': 'VBN', 'dependency': 'conj', 'head': save, 'suffix': 0, 'prefix': 0, 'len_path_root': 1}\n",
            "{'bias': 1.0, 'text': 'the', 'lemma': 'the', 'fine_pos': 'DET', 'coarse_pos': 'DT', 'dependency': 'det', 'head': table, 'suffix': 0, 'prefix': 0, 'len_path_root': 4}\n",
            "{'bias': 1.0, 'text': 'breakfast', 'lemma': 'breakfast', 'fine_pos': 'NOUN', 'coarse_pos': 'NN', 'dependency': 'compound', 'head': table, 'suffix': 0, 'prefix': 0, 'len_path_root': 4}\n",
            "{'bias': 1.0, 'text': 'table', 'lemma': 'table', 'fine_pos': 'NOUN', 'coarse_pos': 'NN', 'dependency': 'pobj', 'head': at, 'suffix': 0, 'prefix': 0, 'len_path_root': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CRF model"
      ],
      "metadata": {
        "id": "u4mqruSLxCVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#second version untested\n",
        "# crf = sklearn_crfsuite.CRF(\n",
        "#     algorithm='lbfgs',\n",
        "#     c1=0.1,\n",
        "#     c2=0.1,\n",
        "#     max_iterations=100,\n",
        "#     all_possible_transitions=True\n",
        "# )\n",
        "# crf.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "-KLsq4QqODPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='l2sgd',\n",
        "    c2=0.1,\n",
        "    max_iterations=1000,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbMuIqv2xdNF",
        "outputId": "d55f2933-185b-4c0c-bf43-f8d7a4f1daaf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRF(algorithm='l2sgd', all_possible_transitions=True, c2=0.1,\n",
              "    max_iterations=1000)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['B-NEG', 'I-NEG', 'O']\n",
        "y_pred = crf.predict(x_test)\n",
        "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9AVf2MVqsX7",
        "outputId": "19ea14df-dbb9-4635-f8e5-76a0122da1f4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9658948245736482"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_labels = sorted(\n",
        "    labels,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        ")\n",
        "print(metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=sorted_labels, digits=3\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJgUmTpi7BIp",
        "outputId": "e194a206-f821-4120-f481-ae31c88fe5a5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           O      0.998     0.952     0.974     13385\n",
            "       B-NEG      0.201     0.869     0.327       176\n",
            "       I-NEG      0.050     0.667     0.093         3\n",
            "\n",
            "    accuracy                          0.951     13564\n",
            "   macro avg      0.417     0.829     0.465     13564\n",
            "weighted avg      0.988     0.951     0.966     13564\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Dyc_QQ8XSKgt",
        "X7d-MsmDOeCX",
        "JcQPgzSOkyCZ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}