{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s4fqP9NQyXhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a303e4ed-72bb-4099-9d8b-4f2c765d114e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn_crfsuite\n",
            "  Cloning https://github.com/MeMartijn/updated-sklearn-crfsuite.git to /tmp/pip-install-0ztmtzbt/sklearn-crfsuite_a77ab6a9bd1b47dfb5aaa151840d09f6\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MeMartijn/updated-sklearn-crfsuite.git /tmp/pip-install-0ztmtzbt/sklearn-crfsuite_a77ab6a9bd1b47dfb5aaa151840d09f6\n",
            "  Resolved https://github.com/MeMartijn/updated-sklearn-crfsuite.git to commit 675038761b4405f04691a83339d04903790e2b95\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.8/dist-packages (from sklearn_crfsuite) (4.64.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sklearn_crfsuite) (1.15.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from sklearn_crfsuite) (0.8.10)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "  Downloading python_crfsuite-0.9.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sklearn_crfsuite\n",
            "  Building wheel for sklearn_crfsuite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn_crfsuite: filename=sklearn_crfsuite-0.3.6-py2.py3-none-any.whl size=10889 sha256=d196a20b5023749aaaf1849bafae902f14e08f514f773b50537442fd4ded6d01\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2ggv8bmb/wheels/fb/c8/8a/95b4eccd3a273adbfb0a08d7f0e96d45d9f4aee82015d293c2\n",
            "Successfully built sklearn_crfsuite\n",
            "Installing collected packages: python-crfsuite, sklearn_crfsuite\n",
            "Successfully installed python-crfsuite-0.9.9 sklearn_crfsuite-0.3.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/MeMartijn/updated-sklearn-crfsuite.git#egg=sklearn_crfsuite\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "#stop_words = set(stopwords.words('english'))\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "from itertools import chain\n",
        "import sklearn\n",
        "import scipy.stats\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
        "\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
      ],
      "metadata": {
        "id": "sOvmxpHGxofh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e15eb70f-960c-47b6-e7d5-3a3c410edb3f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The scikit-learn version is 1.0.2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bhxTZqZuwKwr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5fcae30-273f-4cfd-9285-ecafffcbe267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prvWX5nNORf4"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ndmr_Jagv6wi"
      },
      "outputs": [],
      "source": [
        "def get_root(phrase):\n",
        "  for token in phrase:\n",
        "    if token.dep_ == 'ROOT':\n",
        "      return token\n",
        "\n",
        "#takes a phrase as input in the tokenized fromat\n",
        "#breadth-first search of the dependency tree of the given phrase \n",
        "#returns a the dictionary with the length of the path to root for each token in the sentence     \n",
        "def len_path_root(phrase):\n",
        "  dist = 0\n",
        "  root_token = get_root(phrase)\n",
        "  children = list(root_token.children)\n",
        "  lengths = {\n",
        "      root_token: dist\n",
        "  }\n",
        "  children_count = len(list(children))\n",
        "  while children_count != 0:\n",
        "    dist += 1\n",
        "    new_children = []\n",
        "    for token in children:\n",
        "      lengths[token] = dist\n",
        "      new_children.extend(token.children)\n",
        "    children=new_children\n",
        "    children_count = len(list(new_children))\n",
        "  \n",
        "  #assign length -1 (chosen randomly) for nodes not connected to the root \n",
        "  for token in phrase:\n",
        "    if token not in lengths:\n",
        "      lengths[token] = -1\n",
        "  \n",
        "  return lengths\n",
        "\n",
        "#takes the dataset, phrase_id and chapter_id\n",
        "#retruns the phrase in a string by merging the words\n",
        "#used to iterate in the training and testing set for extracting phrases \n",
        "def get_text(table, id,chapter):\n",
        "  phrase_table = table[(table['phrase_id'] == id) & (table['chapter_id'] == chapter)]\n",
        "  return ' '.join(join_punctuation(phrase_table['word'].values))\n",
        "\n",
        "def join_punctuation(seq, characters='.,;?!'):\n",
        "    characters = set(characters)\n",
        "    seq = iter(seq)\n",
        "    current = next(seq)\n",
        "\n",
        "    for nxt in seq:\n",
        "        if nxt in characters:\n",
        "            current += nxt\n",
        "        else:\n",
        "            yield current\n",
        "            current = nxt\n",
        "\n",
        "    yield current\n",
        "\n",
        "# def get_text(table, id,chapter):\n",
        "#   phrase_table = table[(table['phrase_id'] == id) & (table['chapter_id'] == chapter)]\n",
        "#   return ' '.join(phrase_table['word'].values)\n",
        "\n",
        "#returns a list with the labels of a phrase identified with the ch_id, and phr_id\n",
        "def get_labels(table, id,chapter):\n",
        "  phrase_table = table[(table['phrase_id'] == id) & (table['chapter_id'] == chapter)]\n",
        "  return phrase_table['label'].values\n",
        "\n",
        "#adjusted lemmatization for nltk library\n",
        "#offers the POS as a parameter to lemmatization function to make it more precise\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "#used to match the length of the tokenization with the length of the filtered table \n",
        "#initially implemented for the previous assignment of data preprocessing\n",
        "#treats separately some excpetions found\n",
        "def tokenize(arg, ch = 'baskervilles03', ph = 21):\n",
        "\n",
        "  if (ph, ch) == (436, 'wisteria02'):\n",
        "    with arg.retokenize() as retokenizer:\n",
        "      retokenizer.merge(arg[0:2])\n",
        "      retokenizer.merge(arg[4:7])\n",
        "    return arg\n",
        "\n",
        "  if (ph, ch) in [(450, 'cardboard'),(457, 'cardboard')]:\n",
        "    with arg.retokenize() as retokenizer:\n",
        "      retokenizer.merge(arg[2:4])\n",
        "      retokenizer.merge(arg[0:2])\n",
        "    return arg\n",
        "\n",
        "\n",
        "  no_exc = [('baskervilles03', 16), ('baskervilles03', 20), ('baskervilles11', 45), ('baskervilles12', 283), ('baskervilles13', 271), ('baskervilles14', 55)]\n",
        "  retok1_pos = []#for - \n",
        "  retok2_pos = []#for `\n",
        "\n",
        "  #1\n",
        "  shift = 0\n",
        "  cr_pos = 0\n",
        "  for token in arg:\n",
        "    if token.text == '-':\n",
        "      retok1_pos.append(cr_pos)\n",
        "    cr_pos+=1\n",
        "    prev_char = token.text\n",
        "  \n",
        "  for pos in retok1_pos:\n",
        "    with arg.retokenize() as retokenizer:\n",
        "      retokenizer.merge(arg[pos-1-shift:pos+2-shift])\n",
        "      shift += 2 \n",
        "\n",
        "  #2\n",
        "  shift = 0\n",
        "  cr_pos = 0\n",
        "  prev_char = 0\n",
        "  for token in arg:\n",
        "    if token.text =='`' and prev_char == '`':\n",
        "      retok2_pos.append(cr_pos)\n",
        "    cr_pos+=1\n",
        "    prev_char = token.text\n",
        " \n",
        "  for pos in retok2_pos:\n",
        "    with arg.retokenize() as retokenizer:\n",
        "      retokenizer.merge(arg[pos-shift-1:pos+1-shift])\n",
        "      shift += 1\n",
        "\n",
        "  #3\n",
        "  retok2_pos = []\n",
        "  suf = ['66', '86','ve','m']\n",
        "  shift = 0\n",
        "  cr_pos = 0\n",
        "  prev_char = 0\n",
        "  for token in arg:\n",
        "    if token.text in suf and prev_char == \"'\" or token.text == '.' and prev_char == \"No\" and (ch,ph) not in no_exc:\n",
        "      retok2_pos.append(cr_pos)\n",
        "    cr_pos+=1\n",
        "    prev_char = token.text\n",
        " \n",
        "  for pos in retok2_pos:\n",
        "    with arg.retokenize() as retokenizer:\n",
        "      retokenizer.merge(arg[pos-shift-1:pos+1-shift])\n",
        "      shift += 1\n",
        "\n",
        "  if ph in [0,'0']:\n",
        "    with arg.retokenize() as retokenizer:\n",
        "      retokenizer.merge(arg[2:4])\n",
        "\n",
        "  \n",
        "  return arg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyc_QQ8XSKgt"
      },
      "source": [
        "## [Not needed anymore] Testing attributes on individual pre-set phrase before automatically adding to the table dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "GJAAaSh264RX",
        "outputId": "6c074eb9-6af8-42cf-f3ee-abd938674169"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-320a7629260e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#testing if tokenize and data from table have the same length for each phrase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mall_ch_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chapter_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmismatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_ch_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mfilter_ch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chapter_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ],
      "source": [
        "#testing if tokenize and data from table have the same length for each phrase\n",
        "all_ch_ids = train_data['chapter_id'].unique()\n",
        "mismatch=[]\n",
        "for ch in all_ch_ids:\n",
        "  filter_ch = train_data[train_data['chapter_id'] == ch]\n",
        "  all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "  for ph in all_ph_ids:\n",
        "    filter_ph = filter_ch[filter_ch['phrase_id'] == ph]\n",
        "    phrase = get_text(ph, ch)\n",
        "    phr_doc = nlp(phrase)\n",
        "    tok = tokenize(phr_doc, ch, ph)\n",
        "    if len(filter_ph) != len(tok):\n",
        "      mismatch.append((ch,ph))\n",
        "print(len(mismatch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QhrBL7XXqgm"
      },
      "outputs": [],
      "source": [
        "#testing if there is any negation in \n",
        "all_ch_ids = train_data['chapter_id'].unique()\n",
        "mismatch=[]\n",
        "for ch in all_ch_ids:\n",
        "  filter_ch = train_data[train_data['chapter_id'] == ch]\n",
        "  all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "  for ph in all_ph_ids:\n",
        "    filter_ph = filter_ch[filter_ch['phrase_id'] == ph]\n",
        "    phrase = get_text(ph, ch)\n",
        "    phr_doc = nlp(phrase)\n",
        "    tok = tokenize(phr_doc, ch, ph)\n",
        "    if len(filter_ph) != len(tok):\n",
        "      mismatch.append((ch,ph))\n",
        "print(len(mismatch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4jSB2po4otL"
      },
      "outputs": [],
      "source": [
        "#errors\n",
        "phrase = get_text(436, 'wisteria02')\n",
        "phr_doc = nlp(phrase)\n",
        "#toks = tokenize(phr_doc,436)\n",
        "for tok in phr_doc:\n",
        "  print(tok,'\\n')\n",
        "\n",
        "# flag = 0\n",
        "# if(all(x in abc for x in mismatch)):\n",
        "#     flag = 1\n",
        "# print(flag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_moUSU35c59d"
      },
      "outputs": [],
      "source": [
        "#nbor(d) - neighbour in the initial sentence at distance d -/+ -> to left/right\n",
        "phr1 = \"He is interested in learning Natural Language Processing.\"\n",
        "phr2 = \"I stood upon the hearth-rug and picked up the stick which our visitor had left behind him the night before.\"\n",
        "phr3 = \"Gus Proto is a Python developer currently working for a London-based Fintech company\"\n",
        "\n",
        "phr_doc = nlp(phr2)\n",
        "res = len_path_root(phr_doc)\n",
        "\n",
        "for token in phr_doc:\n",
        "  print(token.text, res[token], \"\\n\")\n",
        "\n",
        "displacy.render(phr_doc, style=\"dep\", jupyter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJdzOnF1AGVY"
      },
      "outputs": [],
      "source": [
        "phr4 = \"Mr. Sherlock Holmes , who was usually very late in the mornings , save upon those not infrequent occasions when he was up all night , was seated at the breakfast table .\"\n",
        "phr4 = \"guru99 is a totally new kind of learning experience.\"\n",
        "phr4 = \"The striped bats are hanging on their feet for best\"\n",
        "\n",
        "#phr4 = sent_tokenize(phr4)\n",
        "words_list = nltk.word_tokenize(phr4)\n",
        "print(tokenize(words_list))\n",
        "print(words_list)\n",
        "#adjusted lemma\n",
        "#lemmatizer = WordNetLemmatizer()\n",
        "#print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words_list])\n",
        "\n",
        "#POS 1,2\n",
        "# fine_tags = nltk.pos_tag(words_list)\n",
        "# coarse_tags = nltk.pos_tag(words_list, tagset='universal')\n",
        "# print(fine_tags)\n",
        "# print(coarse_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue9yXRR9-HuZ"
      },
      "outputs": [],
      "source": [
        "id = 15\n",
        "ch = 'baskervilles01'\n",
        "txt = get_text(id,ch)\n",
        "txt = \"I wouldn't do that\"\n",
        "phr_doc = nlp(txt)\n",
        "\n",
        "#print(len(list(phr_doc)))\n",
        "\n",
        "# with phr_doc.retokenize() as retokenizer:\n",
        "#     retokenizer.merge(phr_doc[21:23])\n",
        "\n",
        "#phr_doc = re_tokenize(phr_doc)\n",
        "for token in phr_doc:\n",
        "  print(token)\n",
        "\n",
        "#displacy.render(phr_doc, style=\"dep\", jupyter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLCw19tK1Cvy"
      },
      "outputs": [],
      "source": [
        "#backup\n",
        "def sent2feature(sentence, ch = 'baskervilles03', ph = 21):\n",
        "  sent_feat = []\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  sent_doc = nlp(sentence)\n",
        "  tokens = tokenize(sent_doc, ch, ph)\n",
        "  lengths = len_path_root(tokens)\n",
        "\n",
        "  ord = 0\n",
        "  shift = 0\n",
        "  for tok in tokens:\n",
        "    features = word2feature(tok)\n",
        "    features['len_path_root'] = lengths[tok]\n",
        "    sent_feat.append(features)\n",
        "\n",
        "  return sent_feat\n",
        "\n",
        "def process_data(table):\n",
        "  all_ch_ids = table['chapter_id'].unique()\n",
        "  all_features = []\n",
        "  all_labels = []\n",
        "  for ch in all_ch_ids:\n",
        "    filter_ch = table[table['chapter_id'] == ch]\n",
        "    all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "    for ph in all_ph_ids:\n",
        "      filter_ph = filter_ch[filter_ch['phrase_id'] == ph]\n",
        "      phrase = get_text(table, ph, ch)\n",
        "      labels = get_labels(table, ph, ch)\n",
        "      all_features.append(sent2feature(phrase, ch, ph))\n",
        "      all_labels.append(labels)\n",
        "\n",
        "  return all_features, all_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7d-MsmDOeCX"
      },
      "source": [
        "## Importing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bViK7iTB7jLq"
      },
      "outputs": [],
      "source": [
        "#merging test datasets\n",
        "test_card = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/ATM/SEM-2012-test-cardboard.txt', sep=\"\\t\", header = None)\n",
        "test_circ = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/ATM/SEM-2012-test-circle.txt', sep=\"\\t\", header = None)\n",
        "\n",
        "frames = [test_card, test_circ]\n",
        "test_data = pd.concat(frames)\n",
        "test_data.rename(columns={1: 'phrase_id', 0: 'chapter_id', 2:'word_id', 3:'word', 4:'label'}, inplace=True)\n",
        "#print(test_data.head(40))\n",
        "\n",
        "#train & dev\n",
        "train_data = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/ATM/SEM-2012-training.txt', sep=\"\\t\", header = None)\n",
        "dev = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/ATM/SEM-2012-dev.txt', sep=\"\\t\", header = None)\n",
        "\n",
        "train_data.rename(columns={1: 'phrase_id', 0: 'chapter_id', 2:'word_id', 3:'word', 4:'label'}, inplace=True)\n",
        "dev.rename(columns={1: 'phrase_id', 0: 'chapter_id', 2:'word_id', 3:'word', 4:'label'}, inplace=True)\n",
        "\n",
        "#print(dev.head(10))\n",
        "#counting B-Neg values\n",
        "#print(train_data['label'].value_counts()['B-NEG'])\n",
        "\n",
        "#merged datasets in test_data; train_data and dev separate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcQPgzSOkyCZ"
      },
      "source": [
        "## Dataset exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pch_WI7txN1z",
        "outputId": "7aaca6f2-6380-4fae-f092-b75b4b14b4be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['baskervilles01' 'baskervilles02' 'baskervilles03' 'baskervilles04'\n",
            " 'baskervilles05' 'baskervilles06' 'baskervilles07' 'baskervilles08'\n",
            " 'baskervilles09' 'baskervilles10' 'baskervilles11' 'baskervilles12'\n",
            " 'baskervilles13' 'baskervilles14' 'wisteria01' 'wisteria02']\n"
          ]
        }
      ],
      "source": [
        "all_ch_ids = train_data['chapter_id'].unique()\n",
        "print(all_ch_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr-rHortyOJJ"
      },
      "outputs": [],
      "source": [
        "#number of phrases/chapter\n",
        "all_ch_ids = train_data['chapter_id'].unique()\n",
        "all_features = []\n",
        "all_labels = []\n",
        "for ch in all_ch_ids:\n",
        "  filter_ch = train_data[train_data['chapter_id'] == ch]\n",
        "  #print(len(filter_ch))\n",
        "  all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "  print(len(all_ph_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akPDVdZlkv82",
        "outputId": "f2c11bb0-e6ef-433c-f01f-3a1e203dc850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83\n",
            "2\n",
            "17.961306256860592\n"
          ]
        }
      ],
      "source": [
        "#num chapters training\n",
        "phrase_lengths = []\n",
        "\n",
        "all_ch_ids = train_data['chapter_id'].unique()\n",
        "all_features = []\n",
        "all_labels = []\n",
        "for ch in all_ch_ids:\n",
        "  filter_ch = train_data[train_data['chapter_id'] == ch]\n",
        "  all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "  for ph in all_ph_ids:\n",
        "    filter_ph = filter_ch[filter_ch['phrase_id'] == ph]\n",
        "    phrase_lengths.append(len(filter_ph))\n",
        "\n",
        "print(max(phrase_lengths))\n",
        "print(min(phrase_lengths))\n",
        "print(sum(phrase_lengths) / len(phrase_lengths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G5i7Uo91Gj7",
        "outputId": "37218621-bce0-4383-d902-2a24d9f340cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65451\n",
            "13567\n"
          ]
        }
      ],
      "source": [
        "print(len(train_data))\n",
        "print(len(dev))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4ERpOk813Fz",
        "outputId": "8b99d712-4728-4811-9fc0-cb03af38a2f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['cardboard' 'circle01' 'circle02']\n"
          ]
        }
      ],
      "source": [
        "all_ch_ids = test_data['chapter_id'].unique()\n",
        "print(all_ch_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PV2IjaVb14yp",
        "outputId": "f32bdefd-6a63-4176-870e-b651b31780f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "496\n",
            "371\n",
            "222\n"
          ]
        }
      ],
      "source": [
        "all_ch_ids = test_data['chapter_id'].unique()\n",
        "all_features = []\n",
        "all_labels = []\n",
        "for ch in all_ch_ids:\n",
        "  filter_ch = test_data[test_data['chapter_id'] == ch]\n",
        "  #print(len(filter_ch))\n",
        "  all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "  print(len(all_ph_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRtADJDu19X-",
        "outputId": "c8889f8d-a789-49e5-f7c9-3ed5da4a30df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "68\n",
            "2\n",
            "17.6455463728191\n"
          ]
        }
      ],
      "source": [
        "phrase_lengths = []\n",
        "\n",
        "all_ch_ids = test_data['chapter_id'].unique()\n",
        "all_features = []\n",
        "all_labels = []\n",
        "for ch in all_ch_ids:\n",
        "  filter_ch = test_data[test_data['chapter_id'] == ch]\n",
        "  all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "  for ph in all_ph_ids:\n",
        "    filter_ph = filter_ch[filter_ch['phrase_id'] == ph]\n",
        "    phrase_lengths.append(len(filter_ph))\n",
        "\n",
        "print(max(phrase_lengths))\n",
        "print(min(phrase_lengths))\n",
        "print(sum(phrase_lengths) / len(phrase_lengths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQWknNlg6j6r"
      },
      "outputs": [],
      "source": [
        "print(list(train_data['label'].values).count('B-NEG'))\n",
        "print(list(train_data['label'].values).count('I-NEG'))\n",
        "print(list(train_data['label'].values).count('O'))\n",
        "\n",
        "print('\\n', \"dev\")\n",
        "print(list(dev['label'].values).count('B-NEG'))\n",
        "print(list(dev['label'].values).count('I-NEG'))\n",
        "print(list(dev['label'].values).count('O'))\n",
        "\n",
        "print('\\n', \"test\")\n",
        "print(list(test_data['label'].values).count('B-NEG'))\n",
        "print(list(test_data['label'].values).count('I-NEG'))\n",
        "print(list(test_data['label'].values).count('O'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR5-pL5miPhX"
      },
      "source": [
        "## CRF functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lXnUKNHQ-fAE"
      },
      "outputs": [],
      "source": [
        "#takes a token as input\n",
        "#returns True if token should be kept, or False if it is filtered\n",
        "#could be changed depending on performance\n",
        "def keep(tok):\n",
        "  neg_list = ['nor', 'Nor', 'neither', 'Neither', 'without', 'Without', 'nobody', 'Nobody', 'none', 'None', 'nothing', 'Nothing', \n",
        "            'never', 'not', 'no', 'Never', 'Not', 'No', 'nowhere', 'non', 'Nowhere', 'Non', \"n't\", \"rather\", \"than\", 'for', 'the']\n",
        "  if tok.text in neg_list:\n",
        "    return True\n",
        "  # if tok.is_punct or tok.is_stop or tok.text == \"``\":\n",
        "  if tok.is_punct or tok.text == \"``\":\n",
        "    return False\n",
        "  return True\n",
        "\n",
        "def word2feature(token):\n",
        "  prefixes = ['un', 'in', 'im','il', 'dis', 'non', 'ir',\n",
        "              'Un', 'In', 'Im','Il', 'Dis', 'Non', 'Ir']\n",
        "  \n",
        "  pref = 0\n",
        "  for p in prefixes:\n",
        "    if token.text.startswith(p):\n",
        "      pref = 1\n",
        "      break\n",
        "  \n",
        "  suf = 0\n",
        "  if 'less' in token.text:\n",
        "    suf = 1\n",
        "  \n",
        "  #for feature selection the unwanted features can be commented\n",
        "  features = {\n",
        "    'text': token.text,\n",
        "    'lemma':token.lemma_,\n",
        "    'fine_pos': token.pos_,\n",
        "    'coarse_pos': token.tag_,\n",
        "    'dependency':token.dep_,\n",
        "    #'head':token.head.text,\n",
        "    'suffix':suf,\n",
        "    'prefix': pref\n",
        "  }\n",
        "\n",
        "  return features\n",
        "\n",
        "#takes as input text of a sentence\n",
        "#returns a list of dictionaries with the features of its tokens\n",
        "def sent2feature(sentence, labels, is_test, ch = 'baskervilles03', ph = 21):\n",
        "  sent_feat = []\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  sent_doc = nlp(sentence)\n",
        "  tokens = tokenize(sent_doc, ch, ph)\n",
        "  lengths = len_path_root(tokens)\n",
        "\n",
        "  ord = 0\n",
        "  shift = 0\n",
        "  for tok in tokens:\n",
        "\n",
        "    if keep(tok) or is_test:\n",
        "      features = word2feature(tok)\n",
        "      #features['len_path_root'] = lengths[tok]\n",
        "      sent_feat.append(features)\n",
        "\n",
        "    else:\n",
        "      labels = np.delete(labels, ord-shift)\n",
        "      shift+=1\n",
        "\n",
        "    ord+=1\n",
        "\n",
        "  return sent_feat, labels\n",
        "\n",
        "\n",
        "#takes the table as an input\n",
        "#is_test makes the preporcessing function keep all entries in case of test data \n",
        "#returns the list of lists of dicitionaries with the features\n",
        "#text->phrase->words \n",
        "def process_data(table, is_test):\n",
        "  all_ch_ids = table['chapter_id'].unique()\n",
        "  all_features = []\n",
        "  all_labels = []\n",
        "  for ch in all_ch_ids:\n",
        "    filter_ch = table[table['chapter_id'] == ch]\n",
        "    all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "    for ph in all_ph_ids:\n",
        "      phrase = get_text(table, ph, ch)\n",
        "      labels = get_labels(table, ph, ch)\n",
        "      #filtered\n",
        "      #print(ch,ph)\n",
        "      filt_features, filt_labels = sent2feature(phrase, labels, is_test, ch, ph)\n",
        "      all_features.append(filt_features)\n",
        "      all_labels.append(filt_labels)\n",
        "\n",
        "  return all_features, all_labels\n",
        "\n",
        "#solving exceptions in data\n",
        "def filter(x,y):\n",
        "  count = 0\n",
        "  new_y = []\n",
        "  for sent,lab in zip(x, y):\n",
        "    if len(sent) != len(lab):\n",
        "      count+=1\n",
        "      lab = np.delete(lab, len(lab)-1)\n",
        "    new_y.append(lab)\n",
        "  print(count)\n",
        "  return new_y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing code"
      ],
      "metadata": {
        "id": "JEX7l7tQTeYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train2[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFoTgU82rvD1",
        "outputId": "edd07df0-d1ae-469e-d776-025e5c526ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train), len(y_train))\n",
        "print(len(x_test), len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekJAulp2UkmA",
        "outputId": "7dc61045-3143-4c50-8be5-f22dee945fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3644 3644\n",
            "787 787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # for word in sent:\n",
        "  #   for feat in word:\n",
        "  #     if :\n",
        "  #       print(word, feat)\n",
        "  #       break"
      ],
      "metadata": {
        "id": "KOuWzYSCPbMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JK2JFA2nUKmu"
      },
      "outputs": [],
      "source": [
        "#counting B-Neg from dataset\n",
        "count = 0\n",
        "for sent in y_train:\n",
        "  for tok in sent:\n",
        "    if tok == 'B-NEG':\n",
        "      count+=1\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#iterate through tokens of sentence\n",
        "doc = nlp(\"I don't like apples and pasta.\")\n",
        "for tok in doc:\n",
        "  print(isinstance(tok, spacy.tokens.token.Token))"
      ],
      "metadata": {
        "id": "VJF7g4baGBEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictio = {'text': 'back', \n",
        "          'lemma': 'back', \n",
        "          'fine_pos': 'NOUN', \n",
        "          'coarse_pos': 'NN', \n",
        "          'dependency': 'pobj', \n",
        "          'head': 'with', \n",
        "          'suffix': 0, \n",
        "          'prefix': 0, \n",
        "          'len_path_root': 2} \n",
        "\n",
        "for key in dictio:\n",
        "  print(key,' : ' ,dictio[key])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gim8JSEyjZkw",
        "outputId": "e0dc9c5a-9b2b-44cc-b7a8-48f3b99f67a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text  :  back\n",
            "lemma  :  back\n",
            "fine_pos  :  NOUN\n",
            "coarse_pos  :  NN\n",
            "dependency  :  pobj\n",
            "head  :  with\n",
            "suffix  :  0\n",
            "prefix  :  0\n",
            "len_path_root  :  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIqDLgqrVXS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "840b7069-89ce-471d-d294-3c5ec323bb51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now ADV\n",
            ", PUNCT\n",
            "all DET\n",
            "these DET\n",
            "rooms NOUN\n",
            "are AUX\n",
            "unfurnished VERB\n",
            "and CCONJ\n",
            "unoccupied ADJ\n",
            "so SCONJ\n",
            "that SCONJ\n",
            "his PRON\n",
            "expedition NOUN\n",
            "became VERB\n",
            "more ADV\n",
            "mysterious ADJ\n",
            "than ADP\n",
            "ever ADV\n",
            ". PUNCT\n",
            "{'text': 'Now', 'lemma': 'now', 'fine_pos': 'ADV', 'coarse_pos': 'RB', 'dependency': 'advmod', 'head': 'unfurnished', 'suffix': 0, 'prefix': 0, 'len_path_root': 1} \n",
            "\n",
            "{'text': ',', 'lemma': ',', 'fine_pos': 'PUNCT', 'coarse_pos': ',', 'dependency': 'punct', 'head': 'unfurnished', 'suffix': 0, 'prefix': 0, 'len_path_root': 1} \n",
            "\n",
            "{'text': 'all', 'lemma': 'all', 'fine_pos': 'DET', 'coarse_pos': 'PDT', 'dependency': 'predet', 'head': 'rooms', 'suffix': 0, 'prefix': 0, 'len_path_root': 2} \n",
            "\n",
            "{'text': 'these', 'lemma': 'these', 'fine_pos': 'DET', 'coarse_pos': 'DT', 'dependency': 'det', 'head': 'rooms', 'suffix': 0, 'prefix': 0, 'len_path_root': 2} \n",
            "\n",
            "{'text': 'rooms', 'lemma': 'room', 'fine_pos': 'NOUN', 'coarse_pos': 'NNS', 'dependency': 'nsubjpass', 'head': 'unfurnished', 'suffix': 0, 'prefix': 0, 'len_path_root': 1} \n",
            "\n",
            "{'text': 'are', 'lemma': 'be', 'fine_pos': 'AUX', 'coarse_pos': 'VBP', 'dependency': 'auxpass', 'head': 'unfurnished', 'suffix': 0, 'prefix': 0, 'len_path_root': 1} \n",
            "\n",
            "{'text': 'unfurnished', 'lemma': 'unfurnishe', 'fine_pos': 'VERB', 'coarse_pos': 'VBN', 'dependency': 'ROOT', 'head': 'unfurnished', 'suffix': 0, 'prefix': 1, 'len_path_root': 0} \n",
            "\n",
            "{'text': 'and', 'lemma': 'and', 'fine_pos': 'CCONJ', 'coarse_pos': 'CC', 'dependency': 'cc', 'head': 'unfurnished', 'suffix': 0, 'prefix': 0, 'len_path_root': 1} \n",
            "\n",
            "{'text': 'unoccupied', 'lemma': 'unoccupied', 'fine_pos': 'ADJ', 'coarse_pos': 'JJ', 'dependency': 'conj', 'head': 'unfurnished', 'suffix': 0, 'prefix': 1, 'len_path_root': 1} \n",
            "\n",
            "{'text': 'so', 'lemma': 'so', 'fine_pos': 'SCONJ', 'coarse_pos': 'IN', 'dependency': 'mark', 'head': 'became', 'suffix': 0, 'prefix': 0, 'len_path_root': 2} \n",
            "\n",
            "{'text': 'that', 'lemma': 'that', 'fine_pos': 'SCONJ', 'coarse_pos': 'IN', 'dependency': 'mark', 'head': 'became', 'suffix': 0, 'prefix': 0, 'len_path_root': 2} \n",
            "\n",
            "{'text': 'his', 'lemma': 'his', 'fine_pos': 'PRON', 'coarse_pos': 'PRP$', 'dependency': 'poss', 'head': 'expedition', 'suffix': 0, 'prefix': 0, 'len_path_root': 3} \n",
            "\n",
            "{'text': 'expedition', 'lemma': 'expedition', 'fine_pos': 'NOUN', 'coarse_pos': 'NN', 'dependency': 'nsubj', 'head': 'became', 'suffix': 0, 'prefix': 0, 'len_path_root': 2} \n",
            "\n",
            "{'text': 'became', 'lemma': 'become', 'fine_pos': 'VERB', 'coarse_pos': 'VBD', 'dependency': 'advcl', 'head': 'unfurnished', 'suffix': 0, 'prefix': 0, 'len_path_root': 1} \n",
            "\n",
            "{'text': 'more', 'lemma': 'more', 'fine_pos': 'ADV', 'coarse_pos': 'RBR', 'dependency': 'advmod', 'head': 'mysterious', 'suffix': 0, 'prefix': 0, 'len_path_root': 3} \n",
            "\n",
            "{'text': 'mysterious', 'lemma': 'mysterious', 'fine_pos': 'ADJ', 'coarse_pos': 'JJ', 'dependency': 'acomp', 'head': 'became', 'suffix': 0, 'prefix': 0, 'len_path_root': 2} \n",
            "\n",
            "{'text': 'than', 'lemma': 'than', 'fine_pos': 'ADP', 'coarse_pos': 'IN', 'dependency': 'prep', 'head': 'mysterious', 'suffix': 0, 'prefix': 0, 'len_path_root': 3} \n",
            "\n",
            "{'text': 'ever', 'lemma': 'ever', 'fine_pos': 'ADV', 'coarse_pos': 'RB', 'dependency': 'pcomp', 'head': 'than', 'suffix': 0, 'prefix': 0, 'len_path_root': 4} \n",
            "\n",
            "{'text': '.', 'lemma': '.', 'fine_pos': 'PUNCT', 'coarse_pos': '.', 'dependency': 'punct', 'head': 'unfurnished', 'suffix': 0, 'prefix': 0, 'len_path_root': 1} \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#extracting features for a single sentence\n",
        "phr1 = get_text(train_data, 121, 'baskervilles08')\n",
        "lab1 = get_labels(train_data, 121, 'baskervilles08')\n",
        "doc = nlp(phr1)\n",
        "doc = tokenize(doc)\n",
        "\n",
        "#printing tokens\n",
        "for tok in doc:\n",
        "  print(tok.text, tok.pos_)\n",
        "\n",
        "#printing extracted features\n",
        "ld = sent2feature(phr1, lab1, 'baskervilles08', 121)\n",
        "for word_dict in ld[0]:\n",
        "  print(word_dict, '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CRF tutorial with original code"
      ],
      "metadata": {
        "id": "_qYZR8KdAIKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'postag': postag,\n",
        "        'postag[:2]': postag[:2],\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            '-1:postag': postag1,\n",
        "            '-1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:postag': postag1,\n",
        "            '+1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, postag, label in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, postag, label in sent]"
      ],
      "metadata": {
        "id": "crhX1ImazeoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('conll2002')\n",
        "nltk.corpus.conll2002.fileids()\n",
        "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
        "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))\n",
        "X_train = [sent2features(s) for s in train_sents]\n",
        "Y_train = [sent2labels(s) for s in train_sents]\n",
        "\n",
        "X_test = [sent2features(s) for s in test_sents]\n",
        "Y_test = [sent2labels(s) for s in test_sents]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4wFLUZVzUeG",
        "outputId": "809847ff-f45e-42fc-ad28-15351a8dbc80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2002 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y_train[7])"
      ],
      "metadata": {
        "id": "oa-7TOjF4z2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[7])"
      ],
      "metadata": {
        "id": "Ia613-BS5ILI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(X_train[7])\n",
        "for feat in X_train[7]:\n",
        "  print(feat)"
      ],
      "metadata": {
        "id": "IFs2eTVl0LwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for feat in x_train[1]:\n",
        "  print(feat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b1TOR7j2QBr",
        "outputId": "2201be57-7944-407c-e2b4-0df0f736c5b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bias': 1.0, 'text': 'Mr.', 'lemma': 'Mr.', 'fine_pos': 'PROPN', 'coarse_pos': 'NNP', 'dependency': 'compound', 'head': Holmes, 'suffix': 0, 'prefix': 0, 'len_path_root': 2}\n",
            "{'bias': 1.0, 'text': 'Sherlock', 'lemma': 'Sherlock', 'fine_pos': 'PROPN', 'coarse_pos': 'NNP', 'dependency': 'compound', 'head': Holmes, 'suffix': 0, 'prefix': 0, 'len_path_root': 2}\n",
            "{'bias': 1.0, 'text': 'Holmes', 'lemma': 'Holmes', 'fine_pos': 'PROPN', 'coarse_pos': 'NNP', 'dependency': 'nsubj', 'head': save, 'suffix': 0, 'prefix': 0, 'len_path_root': 1}\n",
            "{'bias': 1.0, 'text': 'usually', 'lemma': 'usually', 'fine_pos': 'ADV', 'coarse_pos': 'RB', 'dependency': 'advmod', 'head': was, 'suffix': 0, 'prefix': 0, 'len_path_root': 3}\n",
            "{'bias': 1.0, 'text': 'late', 'lemma': 'late', 'fine_pos': 'ADV', 'coarse_pos': 'RB', 'dependency': 'acomp', 'head': was, 'suffix': 0, 'prefix': 0, 'len_path_root': 3}\n",
            "{'bias': 1.0, 'text': 'the', 'lemma': 'the', 'fine_pos': 'DET', 'coarse_pos': 'DT', 'dependency': 'det', 'head': mornings, 'suffix': 0, 'prefix': 0, 'len_path_root': 5}\n",
            "{'bias': 1.0, 'text': 'mornings', 'lemma': 'morning', 'fine_pos': 'NOUN', 'coarse_pos': 'NNS', 'dependency': 'pobj', 'head': in, 'suffix': 0, 'prefix': 0, 'len_path_root': 4}\n",
            "{'bias': 1.0, 'text': 'save', 'lemma': 'save', 'fine_pos': 'VERB', 'coarse_pos': 'VB', 'dependency': 'ROOT', 'head': save, 'suffix': 0, 'prefix': 0, 'len_path_root': 0}\n",
            "{'bias': 1.0, 'text': 'not', 'lemma': 'not', 'fine_pos': 'PART', 'coarse_pos': 'RB', 'dependency': 'neg', 'head': occasions, 'suffix': 0, 'prefix': 0, 'len_path_root': 3}\n",
            "{'bias': 1.0, 'text': 'infrequent', 'lemma': 'infrequent', 'fine_pos': 'ADJ', 'coarse_pos': 'JJ', 'dependency': 'amod', 'head': occasions, 'suffix': 0, 'prefix': 1, 'len_path_root': 3}\n",
            "{'bias': 1.0, 'text': 'occasions', 'lemma': 'occasion', 'fine_pos': 'NOUN', 'coarse_pos': 'NNS', 'dependency': 'pobj', 'head': upon, 'suffix': 0, 'prefix': 0, 'len_path_root': 2}\n",
            "{'bias': 1.0, 'text': 'night', 'lemma': 'night', 'fine_pos': 'NOUN', 'coarse_pos': 'NN', 'dependency': 'npadvmod', 'head': was, 'suffix': 0, 'prefix': 0, 'len_path_root': 2}\n",
            "{'bias': 1.0, 'text': 'seated', 'lemma': 'seat', 'fine_pos': 'VERB', 'coarse_pos': 'VBN', 'dependency': 'conj', 'head': save, 'suffix': 0, 'prefix': 0, 'len_path_root': 1}\n",
            "{'bias': 1.0, 'text': 'the', 'lemma': 'the', 'fine_pos': 'DET', 'coarse_pos': 'DT', 'dependency': 'det', 'head': table, 'suffix': 0, 'prefix': 0, 'len_path_root': 4}\n",
            "{'bias': 1.0, 'text': 'breakfast', 'lemma': 'breakfast', 'fine_pos': 'NOUN', 'coarse_pos': 'NN', 'dependency': 'compound', 'head': table, 'suffix': 0, 'prefix': 0, 'len_path_root': 4}\n",
            "{'bias': 1.0, 'text': 'table', 'lemma': 'table', 'fine_pos': 'NOUN', 'coarse_pos': 'NN', 'dependency': 'pobj', 'head': at, 'suffix': 0, 'prefix': 0, 'len_path_root': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing CRF model"
      ],
      "metadata": {
        "id": "u4mqruSLxCVO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y-kxBedey9V"
      },
      "outputs": [],
      "source": [
        "#data final form\n",
        "x_train, y_trainin = process_data(train_data, False)\n",
        "x_test, y_test = process_data(dev, True)\n",
        "\n",
        "y_train = filter(x_train,y_trainin)\n",
        "y_test = filter(x_test,y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1"
      ],
      "metadata": {
        "id": "ih0j55T1biau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='l2sgd',\n",
        "    c2=0.1,\n",
        "    max_iterations=1000,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "dbMuIqv2xdNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['O', 'B-NEG', 'I-NEG']\n",
        "y_pred = crf.predict(x_test)\n",
        "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9AVf2MVqsX7",
        "outputId": "f4ddfa6f-d5c2-4c7e-a525-2487e2c75caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9968915286964345"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0]))\n",
        "print(metrics.flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJgUmTpi7BIp",
        "outputId": "29065e1c-51cc-4492-f310-c3231a1f2b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           O      0.998     0.999     0.999     13385\n",
            "       B-NEG      0.927     0.864     0.894       176\n",
            "       I-NEG      1.000     0.667     0.800         3\n",
            "\n",
            "    accuracy                          0.997     13564\n",
            "   macro avg      0.975     0.843     0.898     13564\n",
            "weighted avg      0.997     0.997     0.997     13564\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 2"
      ],
      "metadata": {
        "id": "XhiEkXlkbbF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1PW4syxbNZC",
        "outputId": "f1ca7d86-ba41-4361-a301-843085b6bf85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
              "    max_iterations=100)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['O', 'B-NEG', 'I-NEG']\n",
        "y_pred = crf.predict(x_test)\n",
        "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVuoC404bWED",
        "outputId": "58aeb064-9769-4d03-8942-5654d2adbfbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9972186438704623"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0]))\n",
        "print(metrics.flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu_3A2GobVQK",
        "outputId": "a05eedc4-e39a-4de9-8a66-f292907c6295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           O      0.998     0.999     0.999     13385\n",
            "       B-NEG      0.927     0.864     0.894       176\n",
            "       I-NEG      1.000     0.667     0.800         3\n",
            "\n",
            "    accuracy                          0.997     13564\n",
            "   macro avg      0.975     0.843     0.898     13564\n",
            "weighted avg      0.997     0.997     0.997     13564\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning on model 2"
      ],
      "metadata": {
        "id": "HdkihGi-cNDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['O', 'B-NEG', 'I-NEG']\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "\n",
        "params_space = {\n",
        "    'c1': scipy.stats.expon(scale=0.5),\n",
        "    'c2': scipy.stats.expon(scale=0.05),\n",
        "}\n",
        "\n",
        "# use the same metric for evaluation\n",
        "f1_scorer = make_scorer(metrics.flat_f1_score, average='weighted', labels=labels)\n",
        "\n",
        "# search 100 iter\n",
        "rs = RandomizedSearchCV(crf, params_space,\n",
        "                        cv=5,\n",
        "                        verbose=1,\n",
        "                        n_jobs=-1,\n",
        "                        n_iter=100,\n",
        "                        scoring=f1_scorer)\n",
        "rs.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMpqaP7DcPND",
        "outputId": "c7fdd004-1fbb-4453-bad6-8e935ce8c5c9"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=5,\n",
              "                   estimator=CRF(algorithm='lbfgs',\n",
              "                                 all_possible_transitions=True,\n",
              "                                 max_iterations=100),\n",
              "                   n_iter=100, n_jobs=-1,\n",
              "                   param_distributions={'c1': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f23479c0a90>,\n",
              "                                        'c2': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f23479c0250>},\n",
              "                   scoring=make_scorer(flat_f1_score, average=weighted, labels=['O', 'B-NEG', 'I-NEG']),\n",
              "                   verbose=1)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('best params:', rs.best_params_)\n",
        "print('best CV score:', rs.best_score_)\n",
        "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMZ400qqeNea",
        "outputId": "05703e62-ed50-45e7-bd06-2e6242a2c73e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best params: {'c1': 0.019826668862551713, 'c2': 0.029294852587769926}\n",
            "best CV score: 0.9967361262031333\n",
            "model size: 0.10M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final testing set - We will use hypertuned model from above"
      ],
      "metadata": {
        "id": "p5RWWHAVcHVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test, unf_ytest = process_data(test_data, True)\n",
        "y_test = filter(x_test,unf_ytest)"
      ],
      "metadata": {
        "id": "mt9HqBZkhfhQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecefb287-b57d-4da8-fdbc-81cf0cf0af0f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crf = rs.best_estimator_\n",
        "y_pred = crf.predict(x_test)\n",
        "sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0]))\n",
        "print(metrics.flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fM4_NzlfUWOL",
        "outputId": "ace9c86c-d096-46f9-b94d-19ca837c15dc"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           O      0.999     0.998     0.998     18915\n",
            "       B-NEG      0.848     0.911     0.878       269\n",
            "       I-NEG      0.000     0.000     0.000         5\n",
            "\n",
            "    accuracy                          0.996     19189\n",
            "   macro avg      0.615     0.636     0.625     19189\n",
            "weighted avg      0.996     0.996     0.996     19189\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "def print_state_features(state_features):\n",
        "    for (attr, label), weight in state_features:\n",
        "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
        "\n",
        "print(\"Top positive:\")\n",
        "print_state_features(Counter(crf.state_features_).most_common(7))\n",
        "\n",
        "print(\"\\nTop negative:\")\n",
        "print_state_features(Counter(crf.state_features_).most_common()[-7:])"
      ],
      "metadata": {
        "id": "6PDhSQvd9qWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Error Analysis"
      ],
      "metadata": {
        "id": "ZcA09DsWK_Zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#solving exceptions in data\n",
        "#unf, Y_pred\n",
        "def restore(x,y):\n",
        "  count = 0\n",
        "  new_y = []\n",
        "  for sent,lab in zip(x, y):\n",
        "    if len(sent) != len(lab):\n",
        "      lab = np.insert(lab, len(lab),'O')\n",
        "    new_y.append(lab)\n",
        "  return new_y\n",
        "def merged(my_list):\n",
        "  new_format=[]\n",
        "  for llist in my_list:\n",
        "    for el in llist:\n",
        "      new_format.append(el)\n",
        "  new_format = np.array(new_format)\n",
        "  return new_format\n",
        "\n",
        "#lengths y_pred = x_test = y_test\n",
        "# print(len(test_data))\n",
        "\n",
        "# count = 0\n",
        "# for prop in x_test:\n",
        "#   for word in prop:\n",
        "#     count+=1\n",
        "# print(count)"
      ],
      "metadata": {
        "id": "THsFlezOfcjR"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = restore(unf_ytest, y_pred)\n",
        "outputs = merged(y_pred)\n",
        "test_data['prediction'] = outputs\n",
        "non_match = test_data[test_data['prediction'] != test_data['label']]\n",
        "false_neg = non_match[(non_match['prediction']=='O') & (non_match['label']=='B-NEG')]\n",
        "false_pos = non_match[(non_match['label']=='O') & (non_match['prediction']=='B-NEG')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "KV3aIqqar4si",
        "outputId": "00860e22-d547-48b0-e1bb-622258a9fda2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2c074e53888a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munf_ytest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnon_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfalse_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_match\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_match\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'O'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnon_match\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'B-NEG'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'restore' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(false_neg))\n",
        "print(len(false_pos))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zesaUL6zQoSr",
        "outputId": "6d44bfb7-20b3-4018-a830-d04fa9280a09"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n",
            "43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(false_neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "id": "AYJRmMCkgnEA",
        "outputId": "ecc6a39c-f0ba-4390-c5c4-888ad4e8ac36"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      chapter_id  phrase_id  word_id          word  label prediction\n",
              "347    cardboard         12       32      unsolved  B-NEG          O\n",
              "589    cardboard         23       17   incredulity  B-NEG          O\n",
              "669    cardboard         27        4           far  B-NEG          O\n",
              "751    cardboard         31        5     injustice  B-NEG          O\n",
              "904    cardboard         41        6      unframed  B-NEG          O\n",
              "3317   cardboard        167        9   discoloured  B-NEG          O\n",
              "6238   cardboard        322       29   undoubtedly  B-NEG          O\n",
              "6467   cardboard        330       17      unlikely  B-NEG          O\n",
              "6480   cardboard        331        1  unsuccessful  B-NEG          O\n",
              "7962   cardboard        395       31         never  B-NEG          O\n",
              "8377   cardboard        415       11   inseparable  B-NEG          O\n",
              "8942   cardboard        441        6           n't  B-NEG          O\n",
              "9141   cardboard        448        9           not  B-NEG          O\n",
              "9346   cardboard        454       21           not  B-NEG          O\n",
              "10157  cardboard        493       18   unthinkable  B-NEG          O\n",
              "8       circle01          0        8           not  B-NEG          O\n",
              "17      circle01          0       17    uneasiness  B-NEG          O\n",
              "19      circle01          0       19           nor  B-NEG          O\n",
              "1452    circle01        104       11    absolutely  B-NEG          O\n",
              "2141    circle01        149       15       unusual  B-NEG          O\n",
              "7638    circle02        161        7            no  B-NEG          O\n",
              "7711    circle02        164        7       dislike  B-NEG          O\n",
              "7725    circle02        165       12       dislike  B-NEG          O\n",
              "8175    circle02        182        3     senseless  B-NEG          O"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-837ae1a3-9036-44ae-abd6-660b9d48f410\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chapter_id</th>\n",
              "      <th>phrase_id</th>\n",
              "      <th>word_id</th>\n",
              "      <th>word</th>\n",
              "      <th>label</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>cardboard</td>\n",
              "      <td>12</td>\n",
              "      <td>32</td>\n",
              "      <td>unsolved</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>589</th>\n",
              "      <td>cardboard</td>\n",
              "      <td>23</td>\n",
              "      <td>17</td>\n",
              "      <td>incredulity</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>669</th>\n",
              "      <td>cardboard</td>\n",
              "      <td>27</td>\n",
              "      <td>4</td>\n",
              "      <td>far</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>751</th>\n",
              "      <td>cardboard</td>\n",
              "      <td>31</td>\n",
              "      <td>5</td>\n",
              "      <td>injustice</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>904</th>\n",
              "      <td>cardboard</td>\n",
              "      <td>41</td>\n",
              "      <td>6</td>\n",
              "      <td>unframed</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3317</th>\n",
              "      <td>cardboard</td>\n",
              "      <td>167</td>\n",
              "      <td>9</td>\n",
              "      <td>discoloured</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6238</th>\n",
              "      <td>cardboard</td>\n",
              "      <td>322</td>\n",
              "      <td>29</td>\n",
              "      <td>undoubtedly</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6467</th>\n",
              "      <td>cardboard</td>\n",
              "      <td>330</td>\n",
              "      <td>17</td>\n",
              "      <td>unlikely</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6480</th>\n",
              "      <td>cardboard</td>\n",
              "      <td>331</td>\n",
              "      <td>1</td>\n",
              "      <td>unsuccessful</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7962</th>\n",
              "      <td>cardboard</td>\n",
              "      <td>395</td>\n",
              "      <td>31</td>\n",
              "      <td>never</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8377</th>\n",
              "      <td>cardboard</td>\n",
              "      <td>415</td>\n",
              "      <td>11</td>\n",
              "      <td>inseparable</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8942</th>\n",
              "      <td>cardboard</td>\n",
              "      <td>441</td>\n",
              "      <td>6</td>\n",
              "      <td>n't</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9141</th>\n",
              "      <td>cardboard</td>\n",
              "      <td>448</td>\n",
              "      <td>9</td>\n",
              "      <td>not</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9346</th>\n",
              "      <td>cardboard</td>\n",
              "      <td>454</td>\n",
              "      <td>21</td>\n",
              "      <td>not</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10157</th>\n",
              "      <td>cardboard</td>\n",
              "      <td>493</td>\n",
              "      <td>18</td>\n",
              "      <td>unthinkable</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>circle01</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>not</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>circle01</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>uneasiness</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>circle01</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>nor</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1452</th>\n",
              "      <td>circle01</td>\n",
              "      <td>104</td>\n",
              "      <td>11</td>\n",
              "      <td>absolutely</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2141</th>\n",
              "      <td>circle01</td>\n",
              "      <td>149</td>\n",
              "      <td>15</td>\n",
              "      <td>unusual</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7638</th>\n",
              "      <td>circle02</td>\n",
              "      <td>161</td>\n",
              "      <td>7</td>\n",
              "      <td>no</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7711</th>\n",
              "      <td>circle02</td>\n",
              "      <td>164</td>\n",
              "      <td>7</td>\n",
              "      <td>dislike</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7725</th>\n",
              "      <td>circle02</td>\n",
              "      <td>165</td>\n",
              "      <td>12</td>\n",
              "      <td>dislike</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8175</th>\n",
              "      <td>circle02</td>\n",
              "      <td>182</td>\n",
              "      <td>3</td>\n",
              "      <td>senseless</td>\n",
              "      <td>B-NEG</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-837ae1a3-9036-44ae-abd6-660b9d48f410')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-837ae1a3-9036-44ae-abd6-660b9d48f410 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-837ae1a3-9036-44ae-abd6-660b9d48f410');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "prvWX5nNORf4",
        "Dyc_QQ8XSKgt",
        "X7d-MsmDOeCX",
        "JcQPgzSOkyCZ",
        "RR5-pL5miPhX",
        "JEX7l7tQTeYY",
        "_qYZR8KdAIKq"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}