{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4fqP9NQyXhg",
        "outputId": "b0ad68fa-eeed-4eed-b874-fdea66814e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn_crfsuite\n",
            "  Cloning https://github.com/MeMartijn/updated-sklearn-crfsuite.git to /tmp/pip-install-jemzg654/sklearn-crfsuite_cef0c8122e2e462c9f34b380612977ba\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MeMartijn/updated-sklearn-crfsuite.git /tmp/pip-install-jemzg654/sklearn-crfsuite_cef0c8122e2e462c9f34b380612977ba\n",
            "  Resolved https://github.com/MeMartijn/updated-sklearn-crfsuite.git to commit 675038761b4405f04691a83339d04903790e2b95\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.8/dist-packages (from sklearn_crfsuite) (4.64.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sklearn_crfsuite) (1.15.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from sklearn_crfsuite) (0.8.10)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "  Downloading python_crfsuite-0.9.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sklearn_crfsuite\n",
            "  Building wheel for sklearn_crfsuite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn_crfsuite: filename=sklearn_crfsuite-0.3.6-py2.py3-none-any.whl size=10889 sha256=feebd6f9966a8d03f23d84dd8e9dcbd6e2a50dd673c54fa422ef81bd22e61326\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vtum9uf5/wheels/fb/c8/8a/95b4eccd3a273adbfb0a08d7f0e96d45d9f4aee82015d293c2\n",
            "Successfully built sklearn_crfsuite\n",
            "Installing collected packages: python-crfsuite, sklearn_crfsuite\n",
            "Successfully installed python-crfsuite-0.9.8 sklearn_crfsuite-0.3.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/MeMartijn/updated-sklearn-crfsuite.git#egg=sklearn_crfsuite\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "#stop_words = set(stopwords.words('english'))\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "from itertools import chain\n",
        "import sklearn\n",
        "import scipy.stats\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
        "\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOvmxpHGxofh",
        "outputId": "790f14e5-3b9c-464f-ac3d-bc7df2ee6754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The scikit-learn version is 1.0.2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhxTZqZuwKwr",
        "outputId": "28226351-b9ba-455a-bc27-74c43fb283a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prvWX5nNORf4"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ndmr_Jagv6wi"
      },
      "outputs": [],
      "source": [
        "def get_root(phrase):\n",
        "  for token in phrase:\n",
        "    if token.dep_ == 'ROOT':\n",
        "      return token\n",
        "\n",
        "#takes a phrase as input in the tokenized fromat\n",
        "#breadth-first search of the dependency tree of the given phrase \n",
        "#returns a the dictionary with the length of the path to root for each token in the sentence     \n",
        "def len_path_root(phrase):\n",
        "  dist = 0\n",
        "  root_token = get_root(phrase)\n",
        "  children = list(root_token.children)\n",
        "  lengths = {\n",
        "      root_token: dist\n",
        "  }\n",
        "  children_count = len(list(children))\n",
        "  while children_count != 0:\n",
        "    dist += 1\n",
        "    new_children = []\n",
        "    for token in children:\n",
        "      lengths[token] = dist\n",
        "      new_children.extend(token.children)\n",
        "    children=new_children\n",
        "    children_count = len(list(new_children))\n",
        "  \n",
        "  #assign length -1 (chosen randomly) for nodes not connected to the root \n",
        "  for token in phrase:\n",
        "    if token not in lengths:\n",
        "      lengths[token] = -1\n",
        "  \n",
        "  return lengths\n",
        "\n",
        "#takes the dataset, phrase_id and chapter_id\n",
        "#retruns the phrase in a string by merging the words\n",
        "#used to iterate in the training and testing set for extracting phrases \n",
        "def get_text(table, id,chapter):\n",
        "  phrase_table = table[(table['phrase_id'] == id) & (table['chapter_id'] == chapter)]\n",
        "  return ' '.join(join_punctuation(phrase_table['word'].values))\n",
        "\n",
        "def join_punctuation(seq, characters='.,;?!'):\n",
        "    characters = set(characters)\n",
        "    seq = iter(seq)\n",
        "    current = next(seq)\n",
        "\n",
        "    for nxt in seq:\n",
        "        if nxt in characters:\n",
        "            current += nxt\n",
        "        else:\n",
        "            yield current\n",
        "            current = nxt\n",
        "\n",
        "    yield current\n",
        "\n",
        "# def get_text(table, id,chapter):\n",
        "#   phrase_table = table[(table['phrase_id'] == id) & (table['chapter_id'] == chapter)]\n",
        "#   return ' '.join(phrase_table['word'].values)\n",
        "\n",
        "#returns a list with the labels of a phrase identified with the ch_id, and phr_id\n",
        "def get_labels(table, id,chapter):\n",
        "  phrase_table = table[(table['phrase_id'] == id) & (table['chapter_id'] == chapter)]\n",
        "  return phrase_table['label'].values\n",
        "\n",
        "#adjusted lemmatization for nltk library\n",
        "#offers the POS as a parameter to lemmatization function to make it more precise\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "#used to match the length of the tokenization with the length of the filtered table \n",
        "#initially implemented for the previous assignment of data preprocessing\n",
        "#treats separately some excpetions found\n",
        "def tokenize(arg, ch = 'baskervilles03', ph = 21):\n",
        "\n",
        "  if (ph, ch) == (436, 'wisteria02'):\n",
        "    with arg.retokenize() as retokenizer:\n",
        "      retokenizer.merge(arg[0:2])\n",
        "      retokenizer.merge(arg[4:7])\n",
        "    return arg\n",
        "\n",
        "  if (ph, ch) in [(450, 'cardboard'),(457, 'cardboard')]:\n",
        "    with arg.retokenize() as retokenizer:\n",
        "      retokenizer.merge(arg[2:4])\n",
        "      retokenizer.merge(arg[0:2])\n",
        "    return arg\n",
        "\n",
        "\n",
        "  no_exc = [('baskervilles03', 16), ('baskervilles03', 20), ('baskervilles11', 45), ('baskervilles12', 283), ('baskervilles13', 271), ('baskervilles14', 55)]\n",
        "  retok1_pos = []#for - \n",
        "  retok2_pos = []#for `\n",
        "\n",
        "  #1\n",
        "  shift = 0\n",
        "  cr_pos = 0\n",
        "  for token in arg:\n",
        "    if token.text == '-':\n",
        "      retok1_pos.append(cr_pos)\n",
        "    cr_pos+=1\n",
        "    prev_char = token.text\n",
        "  \n",
        "  for pos in retok1_pos:\n",
        "    with arg.retokenize() as retokenizer:\n",
        "      retokenizer.merge(arg[pos-1-shift:pos+2-shift])\n",
        "      shift += 2 \n",
        "\n",
        "  #2\n",
        "  shift = 0\n",
        "  cr_pos = 0\n",
        "  prev_char = 0\n",
        "  for token in arg:\n",
        "    if token.text =='`' and prev_char == '`':\n",
        "      retok2_pos.append(cr_pos)\n",
        "    cr_pos+=1\n",
        "    prev_char = token.text\n",
        " \n",
        "  for pos in retok2_pos:\n",
        "    with arg.retokenize() as retokenizer:\n",
        "      retokenizer.merge(arg[pos-shift-1:pos+1-shift])\n",
        "      shift += 1\n",
        "\n",
        "  #3\n",
        "  retok2_pos = []\n",
        "  suf = ['66', '86','ve','m']\n",
        "  shift = 0\n",
        "  cr_pos = 0\n",
        "  prev_char = 0\n",
        "  for token in arg:\n",
        "    if token.text in suf and prev_char == \"'\" or token.text == '.' and prev_char == \"No\" and (ch,ph) not in no_exc:\n",
        "      retok2_pos.append(cr_pos)\n",
        "    cr_pos+=1\n",
        "    prev_char = token.text\n",
        " \n",
        "  for pos in retok2_pos:\n",
        "    with arg.retokenize() as retokenizer:\n",
        "      retokenizer.merge(arg[pos-shift-1:pos+1-shift])\n",
        "      shift += 1\n",
        "\n",
        "  if ph in [0,'0']:\n",
        "    with arg.retokenize() as retokenizer:\n",
        "      retokenizer.merge(arg[2:4])\n",
        "\n",
        "  \n",
        "  return arg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyc_QQ8XSKgt"
      },
      "source": [
        "## [Not needed anymore] Testing attributes on individual pre-set phrase before automatically adding to the table dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "GJAAaSh264RX",
        "outputId": "6c074eb9-6af8-42cf-f3ee-abd938674169"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-320a7629260e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#testing if tokenize and data from table have the same length for each phrase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mall_ch_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chapter_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmismatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_ch_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mfilter_ch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chapter_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ],
      "source": [
        "#testing if tokenize and data from table have the same length for each phrase\n",
        "all_ch_ids = train_data['chapter_id'].unique()\n",
        "mismatch=[]\n",
        "for ch in all_ch_ids:\n",
        "  filter_ch = train_data[train_data['chapter_id'] == ch]\n",
        "  all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "  for ph in all_ph_ids:\n",
        "    filter_ph = filter_ch[filter_ch['phrase_id'] == ph]\n",
        "    phrase = get_text(ph, ch)\n",
        "    phr_doc = nlp(phrase)\n",
        "    tok = tokenize(phr_doc, ch, ph)\n",
        "    if len(filter_ph) != len(tok):\n",
        "      mismatch.append((ch,ph))\n",
        "print(len(mismatch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QhrBL7XXqgm"
      },
      "outputs": [],
      "source": [
        "#testing if there is any negation in \n",
        "all_ch_ids = train_data['chapter_id'].unique()\n",
        "mismatch=[]\n",
        "for ch in all_ch_ids:\n",
        "  filter_ch = train_data[train_data['chapter_id'] == ch]\n",
        "  all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "  for ph in all_ph_ids:\n",
        "    filter_ph = filter_ch[filter_ch['phrase_id'] == ph]\n",
        "    phrase = get_text(ph, ch)\n",
        "    phr_doc = nlp(phrase)\n",
        "    tok = tokenize(phr_doc, ch, ph)\n",
        "    if len(filter_ph) != len(tok):\n",
        "      mismatch.append((ch,ph))\n",
        "print(len(mismatch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4jSB2po4otL"
      },
      "outputs": [],
      "source": [
        "#errors\n",
        "phrase = get_text(436, 'wisteria02')\n",
        "phr_doc = nlp(phrase)\n",
        "#toks = tokenize(phr_doc,436)\n",
        "for tok in phr_doc:\n",
        "  print(tok,'\\n')\n",
        "\n",
        "# flag = 0\n",
        "# if(all(x in abc for x in mismatch)):\n",
        "#     flag = 1\n",
        "# print(flag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_moUSU35c59d"
      },
      "outputs": [],
      "source": [
        "#nbor(d) - neighbour in the initial sentence at distance d -/+ -> to left/right\n",
        "phr1 = \"He is interested in learning Natural Language Processing.\"\n",
        "phr2 = \"I stood upon the hearth-rug and picked up the stick which our visitor had left behind him the night before.\"\n",
        "phr3 = \"Gus Proto is a Python developer currently working for a London-based Fintech company\"\n",
        "\n",
        "phr_doc = nlp(phr2)\n",
        "res = len_path_root(phr_doc)\n",
        "\n",
        "for token in phr_doc:\n",
        "  print(token.text, res[token], \"\\n\")\n",
        "\n",
        "displacy.render(phr_doc, style=\"dep\", jupyter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJdzOnF1AGVY"
      },
      "outputs": [],
      "source": [
        "phr4 = \"Mr. Sherlock Holmes , who was usually very late in the mornings , save upon those not infrequent occasions when he was up all night , was seated at the breakfast table .\"\n",
        "phr4 = \"guru99 is a totally new kind of learning experience.\"\n",
        "phr4 = \"The striped bats are hanging on their feet for best\"\n",
        "\n",
        "#phr4 = sent_tokenize(phr4)\n",
        "words_list = nltk.word_tokenize(phr4)\n",
        "print(tokenize(words_list))\n",
        "print(words_list)\n",
        "#adjusted lemma\n",
        "#lemmatizer = WordNetLemmatizer()\n",
        "#print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words_list])\n",
        "\n",
        "#POS 1,2\n",
        "# fine_tags = nltk.pos_tag(words_list)\n",
        "# coarse_tags = nltk.pos_tag(words_list, tagset='universal')\n",
        "# print(fine_tags)\n",
        "# print(coarse_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue9yXRR9-HuZ"
      },
      "outputs": [],
      "source": [
        "id = 15\n",
        "ch = 'baskervilles01'\n",
        "txt = get_text(id,ch)\n",
        "txt = \"I wouldn't do that\"\n",
        "phr_doc = nlp(txt)\n",
        "\n",
        "#print(len(list(phr_doc)))\n",
        "\n",
        "# with phr_doc.retokenize() as retokenizer:\n",
        "#     retokenizer.merge(phr_doc[21:23])\n",
        "\n",
        "#phr_doc = re_tokenize(phr_doc)\n",
        "for token in phr_doc:\n",
        "  print(token)\n",
        "\n",
        "#displacy.render(phr_doc, style=\"dep\", jupyter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLCw19tK1Cvy"
      },
      "outputs": [],
      "source": [
        "#backup\n",
        "def sent2feature(sentence, ch = 'baskervilles03', ph = 21):\n",
        "  sent_feat = []\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  sent_doc = nlp(sentence)\n",
        "  tokens = tokenize(sent_doc, ch, ph)\n",
        "  lengths = len_path_root(tokens)\n",
        "\n",
        "  ord = 0\n",
        "  shift = 0\n",
        "  for tok in tokens:\n",
        "    features = word2feature(tok)\n",
        "    features['len_path_root'] = lengths[tok]\n",
        "    sent_feat.append(features)\n",
        "\n",
        "  return sent_feat\n",
        "\n",
        "def process_data(table):\n",
        "  all_ch_ids = table['chapter_id'].unique()\n",
        "  all_features = []\n",
        "  all_labels = []\n",
        "  for ch in all_ch_ids:\n",
        "    filter_ch = table[table['chapter_id'] == ch]\n",
        "    all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "    for ph in all_ph_ids:\n",
        "      filter_ph = filter_ch[filter_ch['phrase_id'] == ph]\n",
        "      phrase = get_text(table, ph, ch)\n",
        "      labels = get_labels(table, ph, ch)\n",
        "      all_features.append(sent2feature(phrase, ch, ph))\n",
        "      all_labels.append(labels)\n",
        "\n",
        "  return all_features, all_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7d-MsmDOeCX"
      },
      "source": [
        "## Importing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bViK7iTB7jLq"
      },
      "outputs": [],
      "source": [
        "#merging test datasets\n",
        "test_card = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/ATM/SEM-2012-test-cardboard.txt', sep=\"\\t\", header = None)\n",
        "test_circ = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/ATM/SEM-2012-test-circle.txt', sep=\"\\t\", header = None)\n",
        "\n",
        "frames = [test_card, test_circ]\n",
        "test_data = pd.concat(frames)\n",
        "test_data.rename(columns={1: 'phrase_id', 0: 'chapter_id', 2:'word_id', 3:'word', 4:'label'}, inplace=True)\n",
        "#print(test_data.head(40))\n",
        "\n",
        "#train & dev\n",
        "train_data = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/ATM/SEM-2012-training.txt', sep=\"\\t\", header = None)\n",
        "dev = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/ATM/SEM-2012-dev.txt', sep=\"\\t\", header = None)\n",
        "\n",
        "train_data.rename(columns={1: 'phrase_id', 0: 'chapter_id', 2:'word_id', 3:'word', 4:'label'}, inplace=True)\n",
        "dev.rename(columns={1: 'phrase_id', 0: 'chapter_id', 2:'word_id', 3:'word', 4:'label'}, inplace=True)\n",
        "\n",
        "#print(dev.head(10))\n",
        "#counting B-Neg values\n",
        "#print(train_data['label'].value_counts()['B-NEG'])\n",
        "\n",
        "#merged datasets in test_data; train_data and dev separate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcQPgzSOkyCZ"
      },
      "source": [
        "## Dataset exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pch_WI7txN1z",
        "outputId": "7aaca6f2-6380-4fae-f092-b75b4b14b4be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['baskervilles01' 'baskervilles02' 'baskervilles03' 'baskervilles04'\n",
            " 'baskervilles05' 'baskervilles06' 'baskervilles07' 'baskervilles08'\n",
            " 'baskervilles09' 'baskervilles10' 'baskervilles11' 'baskervilles12'\n",
            " 'baskervilles13' 'baskervilles14' 'wisteria01' 'wisteria02']\n"
          ]
        }
      ],
      "source": [
        "all_ch_ids = train_data['chapter_id'].unique()\n",
        "print(all_ch_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr-rHortyOJJ"
      },
      "outputs": [],
      "source": [
        "#number of phrases/chapter\n",
        "all_ch_ids = train_data['chapter_id'].unique()\n",
        "all_features = []\n",
        "all_labels = []\n",
        "for ch in all_ch_ids:\n",
        "  filter_ch = train_data[train_data['chapter_id'] == ch]\n",
        "  #print(len(filter_ch))\n",
        "  all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "  print(len(all_ph_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akPDVdZlkv82",
        "outputId": "f2c11bb0-e6ef-433c-f01f-3a1e203dc850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83\n",
            "2\n",
            "17.961306256860592\n"
          ]
        }
      ],
      "source": [
        "#num chapters training\n",
        "phrase_lengths = []\n",
        "\n",
        "all_ch_ids = train_data['chapter_id'].unique()\n",
        "all_features = []\n",
        "all_labels = []\n",
        "for ch in all_ch_ids:\n",
        "  filter_ch = train_data[train_data['chapter_id'] == ch]\n",
        "  all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "  for ph in all_ph_ids:\n",
        "    filter_ph = filter_ch[filter_ch['phrase_id'] == ph]\n",
        "    phrase_lengths.append(len(filter_ph))\n",
        "\n",
        "print(max(phrase_lengths))\n",
        "print(min(phrase_lengths))\n",
        "print(sum(phrase_lengths) / len(phrase_lengths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G5i7Uo91Gj7",
        "outputId": "37218621-bce0-4383-d902-2a24d9f340cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65451\n",
            "13567\n"
          ]
        }
      ],
      "source": [
        "print(len(train_data))\n",
        "print(len(dev))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4ERpOk813Fz",
        "outputId": "8b99d712-4728-4811-9fc0-cb03af38a2f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['cardboard' 'circle01' 'circle02']\n"
          ]
        }
      ],
      "source": [
        "all_ch_ids = test_data['chapter_id'].unique()\n",
        "print(all_ch_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PV2IjaVb14yp",
        "outputId": "f32bdefd-6a63-4176-870e-b651b31780f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "496\n",
            "371\n",
            "222\n"
          ]
        }
      ],
      "source": [
        "all_ch_ids = test_data['chapter_id'].unique()\n",
        "all_features = []\n",
        "all_labels = []\n",
        "for ch in all_ch_ids:\n",
        "  filter_ch = test_data[test_data['chapter_id'] == ch]\n",
        "  #print(len(filter_ch))\n",
        "  all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "  print(len(all_ph_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRtADJDu19X-",
        "outputId": "c8889f8d-a789-49e5-f7c9-3ed5da4a30df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "68\n",
            "2\n",
            "17.6455463728191\n"
          ]
        }
      ],
      "source": [
        "phrase_lengths = []\n",
        "\n",
        "all_ch_ids = test_data['chapter_id'].unique()\n",
        "all_features = []\n",
        "all_labels = []\n",
        "for ch in all_ch_ids:\n",
        "  filter_ch = test_data[test_data['chapter_id'] == ch]\n",
        "  all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "  for ph in all_ph_ids:\n",
        "    filter_ph = filter_ch[filter_ch['phrase_id'] == ph]\n",
        "    phrase_lengths.append(len(filter_ph))\n",
        "\n",
        "print(max(phrase_lengths))\n",
        "print(min(phrase_lengths))\n",
        "print(sum(phrase_lengths) / len(phrase_lengths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQWknNlg6j6r"
      },
      "outputs": [],
      "source": [
        "print(list(train_data['label'].values).count('B-NEG'))\n",
        "print(list(train_data['label'].values).count('I-NEG'))\n",
        "print(list(train_data['label'].values).count('O'))\n",
        "\n",
        "print('\\n', \"dev\")\n",
        "print(list(dev['label'].values).count('B-NEG'))\n",
        "print(list(dev['label'].values).count('I-NEG'))\n",
        "print(list(dev['label'].values).count('O'))\n",
        "\n",
        "print('\\n', \"test\")\n",
        "print(list(test_data['label'].values).count('B-NEG'))\n",
        "print(list(test_data['label'].values).count('I-NEG'))\n",
        "print(list(test_data['label'].values).count('O'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR5-pL5miPhX"
      },
      "source": [
        "## CRF functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXnUKNHQ-fAE"
      },
      "outputs": [],
      "source": [
        "#takes a token as input\n",
        "#returns True if token should be kept, or False if it is filtered\n",
        "#could be changed depending on performance\n",
        "def keep(tok):\n",
        "  neg_list = ['nor', 'Nor', 'neither', 'Neither', 'without', 'Without', 'nobody', 'Nobody', 'none', 'None', 'nothing', 'Nothing', \n",
        "            'never', 'not', 'no', 'Never', 'Not', 'No', 'nowhere', 'non', 'Nowhere', 'Non', \"n't\", \"rather\", \"than\", 'for', 'the']\n",
        "  if tok.text in neg_list:\n",
        "    return True\n",
        "  # if tok.is_punct or tok.is_stop or tok.text == \"``\":\n",
        "  if tok.is_punct or tok.text == \"``\":\n",
        "    return False\n",
        "  return True\n",
        "\n",
        "def word2feature(token):\n",
        "  prefixes = ['un', 'in', 'im','il', 'dis', 'non', 'ir',\n",
        "              'Un', 'In', 'Im','Il', 'Dis', 'Non', 'Ir']\n",
        "  \n",
        "  pref = 0\n",
        "  for p in prefixes:\n",
        "    if token.text.startswith(p):\n",
        "      pref = 1\n",
        "      break\n",
        "  \n",
        "  suf = 0\n",
        "  if 'less' in token.text:\n",
        "    suf = 1\n",
        "   \n",
        "  features = {\n",
        "    'text': token.text,\n",
        "    'lemma':token.lemma_,\n",
        "    'fine_pos': token.pos_,\n",
        "    'coarse_pos': token.tag_,\n",
        "    'dependency':token.dep_,\n",
        "    'head':token.head.text,\n",
        "    'suffix':suf,\n",
        "    'prefix': pref\n",
        "  }\n",
        "\n",
        "  return features\n",
        "\n",
        "#use this\n",
        "\n",
        "#takes as input text of a sentence\n",
        "#returns a list of dictionaries with the features of its tokens\n",
        "def sent2feature(sentence, labels, is_test, ch = 'baskervilles03', ph = 21):\n",
        "  sent_feat = []\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  sent_doc = nlp(sentence)\n",
        "  tokens = tokenize(sent_doc, ch, ph)\n",
        "  lengths = len_path_root(tokens)\n",
        "\n",
        "  ord = 0\n",
        "  shift = 0\n",
        "  for tok in tokens:\n",
        "\n",
        "    if keep(tok) or is_test:\n",
        "      features = word2feature(tok)\n",
        "      features['len_path_root'] = lengths[tok]\n",
        "      sent_feat.append(features)\n",
        "\n",
        "    else:\n",
        "      labels = np.delete(labels, ord-shift)\n",
        "      shift+=1\n",
        "\n",
        "    ord+=1\n",
        "\n",
        "  return sent_feat, labels\n",
        "\n",
        "#takes the table as an input\n",
        "#is_test makes the preporcessing function keep all entries in case of test data \n",
        "#returns the list of lists of dicitionaries with the features\n",
        "#text->phrase->words \n",
        "def process_data(table, is_test):\n",
        "  all_ch_ids = table['chapter_id'].unique()\n",
        "  all_features = []\n",
        "  all_labels = []\n",
        "  for ch in all_ch_ids:\n",
        "    filter_ch = table[table['chapter_id'] == ch]\n",
        "    all_ph_ids = filter_ch['phrase_id'].unique()\n",
        "    for ph in all_ph_ids:\n",
        "      phrase = get_text(table, ph, ch)\n",
        "      labels = get_labels(table, ph, ch)\n",
        "      #filtered\n",
        "      #print(ch,ph)\n",
        "      filt_features, filt_labels = sent2feature(phrase, labels, is_test, ch, ph)\n",
        "      all_features.append(filt_features)\n",
        "      all_labels.append(filt_labels)\n",
        "\n",
        "  return all_features, all_labels\n",
        "\n",
        "#solving exceptions in data\n",
        "def filter(x,y):\n",
        "  new_y = []\n",
        "  for sent,lab in zip(x, y):\n",
        "    if len(sent) != len(lab):\n",
        "      lab = np.delete(lab, len(lab)-1)\n",
        "    \n",
        "    #tried replacing B-Neg tag with B-LOC, as this belongs to crf.classes_\n",
        "    # repl = []\n",
        "    # for l in lab:\n",
        "    #   if l == 'B-NEG':\n",
        "    #     repl.append('B-LOC')\n",
        "    #   elif l == 'I-NEG':\n",
        "    #     repl.append('I-LOC')\n",
        "    #   else:\n",
        "    #     repl.append(l)\n",
        "    #   lab = repl\n",
        "    #\n",
        "\n",
        "    new_y.append(lab)\n",
        "\n",
        "  return new_y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing code"
      ],
      "metadata": {
        "id": "JEX7l7tQTeYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train2[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFoTgU82rvD1",
        "outputId": "edd07df0-d1ae-469e-d776-025e5c526ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train), len(y_train))\n",
        "print(len(x_test), len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekJAulp2UkmA",
        "outputId": "7dc61045-3143-4c50-8be5-f22dee945fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3644 3644\n",
            "787 787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # for word in sent:\n",
        "  #   for feat in word:\n",
        "  #     if :\n",
        "  #       print(word, feat)\n",
        "  #       break"
      ],
      "metadata": {
        "id": "KOuWzYSCPbMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JK2JFA2nUKmu"
      },
      "outputs": [],
      "source": [
        "#counting B-Neg from dataset\n",
        "count = 0\n",
        "for sent in y_train:\n",
        "  for tok in sent:\n",
        "    if tok == 'B-NEG':\n",
        "      count+=1\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#iterate through tokens of sentence\n",
        "doc = nlp(\"I don't like apples and pasta.\")\n",
        "for tok in doc:\n",
        "  print(isinstance(tok, spacy.tokens.token.Token))"
      ],
      "metadata": {
        "id": "VJF7g4baGBEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictio = {'text': 'back', \n",
        "          'lemma': 'back', \n",
        "          'fine_pos': 'NOUN', \n",
        "          'coarse_pos': 'NN', \n",
        "          'dependency': 'pobj', \n",
        "          'head': 'with', \n",
        "          'suffix': 0, \n",
        "          'prefix': 0, \n",
        "          'len_path_root': 2} \n",
        "\n",
        "for key in dictio:\n",
        "  print(key,' : ' ,dictio[key])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gim8JSEyjZkw",
        "outputId": "e0dc9c5a-9b2b-44cc-b7a8-48f3b99f67a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text  :  back\n",
            "lemma  :  back\n",
            "fine_pos  :  NOUN\n",
            "coarse_pos  :  NN\n",
            "dependency  :  pobj\n",
            "head  :  with\n",
            "suffix  :  0\n",
            "prefix  :  0\n",
            "len_path_root  :  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIqDLgqrVXS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "840b7069-89ce-471d-d294-3c5ec323bb51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now ADV\n",
            ", PUNCT\n",
            "all DET\n",
            "these DET\n",
            "rooms NOUN\n",
            "are AUX\n",
            "unfurnished VERB\n",
            "and CCONJ\n",
            "unoccupied ADJ\n",
            "so SCONJ\n",
            "that SCONJ\n",
            "his PRON\n",
            "expedition NOUN\n",
            "became VERB\n",
            "more ADV\n",
            "mysterious ADJ\n",
            "than ADP\n",
            "ever ADV\n",
            ". PUNCT\n",
            "{'text': 'Now', 'lemma': 'now', 'fine_pos': 'ADV', 'coarse_pos': 'RB', 'dependency': 'advmod', 'head': 'unfurnished', 'suffix': 0, 'prefix': 0, 'len_path_root': 1} \n",
            "\n",
            "{'text': ',', 'lemma': ',', 'fine_pos': 'PUNCT', 'coarse_pos': ',', 'dependency': 'punct', 'head': 'unfurnished', 'suffix': 0, 'prefix': 0, 'len_path_root': 1} \n",
            "\n",
            "{'text': 'all', 'lemma': 'all', 'fine_pos': 'DET', 'coarse_pos': 'PDT', 'dependency': 'predet', 'head': 'rooms', 'suffix': 0, 'prefix': 0, 'len_path_root': 2} \n",
            "\n",
            "{'text': 'these', 'lemma': 'these', 'fine_pos': 'DET', 'coarse_pos': 'DT', 'dependency': 'det', 'head': 'rooms', 'suffix': 0, 'prefix': 0, 'len_path_root': 2} \n",
            "\n",
            "{'text': 'rooms', 'lemma': 'room', 'fine_pos': 'NOUN', 'coarse_pos': 'NNS', 'dependency': 'nsubjpass', 'head': 'unfurnished', 'suffix': 0, 'prefix': 0, 'len_path_root': 1} \n",
            "\n",
            "{'text': 'are', 'lemma': 'be', 'fine_pos': 'AUX', 'coarse_pos': 'VBP', 'dependency': 'auxpass', 'head': 'unfurnished', 'suffix': 0, 'prefix': 0, 'len_path_root': 1} \n",
            "\n",
            "{'text': 'unfurnished', 'lemma': 'unfurnishe', 'fine_pos': 'VERB', 'coarse_pos': 'VBN', 'dependency': 'ROOT', 'head': 'unfurnished', 'suffix': 0, 'prefix': 1, 'len_path_root': 0} \n",
            "\n",
            "{'text': 'and', 'lemma': 'and', 'fine_pos': 'CCONJ', 'coarse_pos': 'CC', 'dependency': 'cc', 'head': 'unfurnished', 'suffix': 0, 'prefix': 0, 'len_path_root': 1} \n",
            "\n",
            "{'text': 'unoccupied', 'lemma': 'unoccupied', 'fine_pos': 'ADJ', 'coarse_pos': 'JJ', 'dependency': 'conj', 'head': 'unfurnished', 'suffix': 0, 'prefix': 1, 'len_path_root': 1} \n",
            "\n",
            "{'text': 'so', 'lemma': 'so', 'fine_pos': 'SCONJ', 'coarse_pos': 'IN', 'dependency': 'mark', 'head': 'became', 'suffix': 0, 'prefix': 0, 'len_path_root': 2} \n",
            "\n",
            "{'text': 'that', 'lemma': 'that', 'fine_pos': 'SCONJ', 'coarse_pos': 'IN', 'dependency': 'mark', 'head': 'became', 'suffix': 0, 'prefix': 0, 'len_path_root': 2} \n",
            "\n",
            "{'text': 'his', 'lemma': 'his', 'fine_pos': 'PRON', 'coarse_pos': 'PRP$', 'dependency': 'poss', 'head': 'expedition', 'suffix': 0, 'prefix': 0, 'len_path_root': 3} \n",
            "\n",
            "{'text': 'expedition', 'lemma': 'expedition', 'fine_pos': 'NOUN', 'coarse_pos': 'NN', 'dependency': 'nsubj', 'head': 'became', 'suffix': 0, 'prefix': 0, 'len_path_root': 2} \n",
            "\n",
            "{'text': 'became', 'lemma': 'become', 'fine_pos': 'VERB', 'coarse_pos': 'VBD', 'dependency': 'advcl', 'head': 'unfurnished', 'suffix': 0, 'prefix': 0, 'len_path_root': 1} \n",
            "\n",
            "{'text': 'more', 'lemma': 'more', 'fine_pos': 'ADV', 'coarse_pos': 'RBR', 'dependency': 'advmod', 'head': 'mysterious', 'suffix': 0, 'prefix': 0, 'len_path_root': 3} \n",
            "\n",
            "{'text': 'mysterious', 'lemma': 'mysterious', 'fine_pos': 'ADJ', 'coarse_pos': 'JJ', 'dependency': 'acomp', 'head': 'became', 'suffix': 0, 'prefix': 0, 'len_path_root': 2} \n",
            "\n",
            "{'text': 'than', 'lemma': 'than', 'fine_pos': 'ADP', 'coarse_pos': 'IN', 'dependency': 'prep', 'head': 'mysterious', 'suffix': 0, 'prefix': 0, 'len_path_root': 3} \n",
            "\n",
            "{'text': 'ever', 'lemma': 'ever', 'fine_pos': 'ADV', 'coarse_pos': 'RB', 'dependency': 'pcomp', 'head': 'than', 'suffix': 0, 'prefix': 0, 'len_path_root': 4} \n",
            "\n",
            "{'text': '.', 'lemma': '.', 'fine_pos': 'PUNCT', 'coarse_pos': '.', 'dependency': 'punct', 'head': 'unfurnished', 'suffix': 0, 'prefix': 0, 'len_path_root': 1} \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#extracting features for a single sentence\n",
        "phr1 = get_text(train_data, 121, 'baskervilles08')\n",
        "lab1 = get_labels(train_data, 121, 'baskervilles08')\n",
        "doc = nlp(phr1)\n",
        "doc = tokenize(doc)\n",
        "\n",
        "#printing tokens\n",
        "for tok in doc:\n",
        "  print(tok.text, tok.pos_)\n",
        "\n",
        "#printing extracted features\n",
        "ld = sent2feature(phr1, lab1, 'baskervilles08', 121)\n",
        "for word_dict in ld[0]:\n",
        "  print(word_dict, '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CRF tutorial with original code"
      ],
      "metadata": {
        "id": "_qYZR8KdAIKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn-crfsuite"
      ],
      "metadata": {
        "id": "dI57q21Jy8vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'postag': postag,\n",
        "        'postag[:2]': postag[:2],\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            '-1:postag': postag1,\n",
        "            '-1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:postag': postag1,\n",
        "            '+1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, postag, label in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, postag, label in sent]"
      ],
      "metadata": {
        "id": "crhX1ImazeoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('conll2002')\n",
        "nltk.corpus.conll2002.fileids()\n",
        "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
        "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))\n",
        "X_train = [sent2features(s) for s in train_sents]\n",
        "Y_train = [sent2labels(s) for s in train_sents]\n",
        "\n",
        "X_test = [sent2features(s) for s in test_sents]\n",
        "Y_test = [sent2labels(s) for s in test_sents]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4wFLUZVzUeG",
        "outputId": "809847ff-f45e-42fc-ad28-15351a8dbc80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2002 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y_train[7])"
      ],
      "metadata": {
        "id": "oa-7TOjF4z2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[7])"
      ],
      "metadata": {
        "id": "Ia613-BS5ILI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(X_train[7])\n",
        "for feat in X_train[7]:\n",
        "  print(feat)"
      ],
      "metadata": {
        "id": "IFs2eTVl0LwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for feat in x_train[1]:\n",
        "  print(feat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b1TOR7j2QBr",
        "outputId": "2201be57-7944-407c-e2b4-0df0f736c5b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bias': 1.0, 'text': 'Mr.', 'lemma': 'Mr.', 'fine_pos': 'PROPN', 'coarse_pos': 'NNP', 'dependency': 'compound', 'head': Holmes, 'suffix': 0, 'prefix': 0, 'len_path_root': 2}\n",
            "{'bias': 1.0, 'text': 'Sherlock', 'lemma': 'Sherlock', 'fine_pos': 'PROPN', 'coarse_pos': 'NNP', 'dependency': 'compound', 'head': Holmes, 'suffix': 0, 'prefix': 0, 'len_path_root': 2}\n",
            "{'bias': 1.0, 'text': 'Holmes', 'lemma': 'Holmes', 'fine_pos': 'PROPN', 'coarse_pos': 'NNP', 'dependency': 'nsubj', 'head': save, 'suffix': 0, 'prefix': 0, 'len_path_root': 1}\n",
            "{'bias': 1.0, 'text': 'usually', 'lemma': 'usually', 'fine_pos': 'ADV', 'coarse_pos': 'RB', 'dependency': 'advmod', 'head': was, 'suffix': 0, 'prefix': 0, 'len_path_root': 3}\n",
            "{'bias': 1.0, 'text': 'late', 'lemma': 'late', 'fine_pos': 'ADV', 'coarse_pos': 'RB', 'dependency': 'acomp', 'head': was, 'suffix': 0, 'prefix': 0, 'len_path_root': 3}\n",
            "{'bias': 1.0, 'text': 'the', 'lemma': 'the', 'fine_pos': 'DET', 'coarse_pos': 'DT', 'dependency': 'det', 'head': mornings, 'suffix': 0, 'prefix': 0, 'len_path_root': 5}\n",
            "{'bias': 1.0, 'text': 'mornings', 'lemma': 'morning', 'fine_pos': 'NOUN', 'coarse_pos': 'NNS', 'dependency': 'pobj', 'head': in, 'suffix': 0, 'prefix': 0, 'len_path_root': 4}\n",
            "{'bias': 1.0, 'text': 'save', 'lemma': 'save', 'fine_pos': 'VERB', 'coarse_pos': 'VB', 'dependency': 'ROOT', 'head': save, 'suffix': 0, 'prefix': 0, 'len_path_root': 0}\n",
            "{'bias': 1.0, 'text': 'not', 'lemma': 'not', 'fine_pos': 'PART', 'coarse_pos': 'RB', 'dependency': 'neg', 'head': occasions, 'suffix': 0, 'prefix': 0, 'len_path_root': 3}\n",
            "{'bias': 1.0, 'text': 'infrequent', 'lemma': 'infrequent', 'fine_pos': 'ADJ', 'coarse_pos': 'JJ', 'dependency': 'amod', 'head': occasions, 'suffix': 0, 'prefix': 1, 'len_path_root': 3}\n",
            "{'bias': 1.0, 'text': 'occasions', 'lemma': 'occasion', 'fine_pos': 'NOUN', 'coarse_pos': 'NNS', 'dependency': 'pobj', 'head': upon, 'suffix': 0, 'prefix': 0, 'len_path_root': 2}\n",
            "{'bias': 1.0, 'text': 'night', 'lemma': 'night', 'fine_pos': 'NOUN', 'coarse_pos': 'NN', 'dependency': 'npadvmod', 'head': was, 'suffix': 0, 'prefix': 0, 'len_path_root': 2}\n",
            "{'bias': 1.0, 'text': 'seated', 'lemma': 'seat', 'fine_pos': 'VERB', 'coarse_pos': 'VBN', 'dependency': 'conj', 'head': save, 'suffix': 0, 'prefix': 0, 'len_path_root': 1}\n",
            "{'bias': 1.0, 'text': 'the', 'lemma': 'the', 'fine_pos': 'DET', 'coarse_pos': 'DT', 'dependency': 'det', 'head': table, 'suffix': 0, 'prefix': 0, 'len_path_root': 4}\n",
            "{'bias': 1.0, 'text': 'breakfast', 'lemma': 'breakfast', 'fine_pos': 'NOUN', 'coarse_pos': 'NN', 'dependency': 'compound', 'head': table, 'suffix': 0, 'prefix': 0, 'len_path_root': 4}\n",
            "{'bias': 1.0, 'text': 'table', 'lemma': 'table', 'fine_pos': 'NOUN', 'coarse_pos': 'NN', 'dependency': 'pobj', 'head': at, 'suffix': 0, 'prefix': 0, 'len_path_root': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing CRF model"
      ],
      "metadata": {
        "id": "u4mqruSLxCVO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y-kxBedey9V"
      },
      "outputs": [],
      "source": [
        "#data final form\n",
        "x_train, y_trainin = process_data(train_data, False)\n",
        "x_test, y_test = process_data(dev, True)\n",
        "\n",
        "y_train = filter(x_train,y_trainin)\n",
        "y_test = filter(x_test,y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1"
      ],
      "metadata": {
        "id": "ih0j55T1biau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='l2sgd',\n",
        "    c2=0.1,\n",
        "    max_iterations=1000,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbMuIqv2xdNF",
        "outputId": "c84198a9-d0e7-487b-c77b-4651f9df1272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRF(algorithm='l2sgd', all_possible_transitions=True, c2=0.1,\n",
              "    max_iterations=1000)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['B-NEG', 'I-NEG']\n",
        "y_pred = crf.predict(x_test)\n",
        "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9AVf2MVqsX7",
        "outputId": "31a96105-1e03-4565-b599-aa6478ba8ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8880513632952555"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0]))\n",
        "print(metrics.flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJgUmTpi7BIp",
        "outputId": "be2339fc-dc21-4b02-bb76-53a45132b887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-NEG      0.937     0.847     0.890       176\n",
            "       I-NEG      1.000     0.667     0.800         3\n",
            "\n",
            "   micro avg      0.938     0.844     0.888       179\n",
            "   macro avg      0.969     0.757     0.845       179\n",
            "weighted avg      0.938     0.844     0.888       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 2"
      ],
      "metadata": {
        "id": "XhiEkXlkbbF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1PW4syxbNZC",
        "outputId": "951bc333-0513-4a9b-8fe5-f2730b0db23e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
              "    max_iterations=100)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['B-NEG', 'I-NEG']\n",
        "y_pred = crf.predict(x_test)\n",
        "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVuoC404bWED",
        "outputId": "149f4417-f323-4c9d-f826-0f070b206aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8893327400669072"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0]))\n",
        "print(metrics.flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu_3A2GobVQK",
        "outputId": "4af5e3ba-cb67-40f5-ee70-05242d86422e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-NEG      0.926     0.858     0.891       176\n",
            "       I-NEG      1.000     0.667     0.800         3\n",
            "\n",
            "   micro avg      0.927     0.855     0.890       179\n",
            "   macro avg      0.963     0.762     0.845       179\n",
            "weighted avg      0.928     0.855     0.889       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tried feature selection\n",
        "# from sklearn.feature_selection import VarianceThreshold\n",
        "# sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
        "# print(sel.fit_transform(x_train))"
      ],
      "metadata": {
        "id": "xwGxr9R_NCSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning on model 2"
      ],
      "metadata": {
        "id": "HdkihGi-cNDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "\n",
        "params_space = {\n",
        "    'c1': scipy.stats.expon(scale=0.5),\n",
        "    'c2': scipy.stats.expon(scale=0.05),\n",
        "}\n",
        "\n",
        "# use the same metric for evaluation\n",
        "f1_scorer = make_scorer(metrics.flat_f1_score, average='weighted', labels=labels)\n",
        "\n",
        "# search 100 iter\n",
        "rs = RandomizedSearchCV(crf, params_space,\n",
        "                        cv=5,\n",
        "                        verbose=1,\n",
        "                        n_jobs=-1,\n",
        "                        n_iter=100,\n",
        "                        scoring=f1_scorer)\n",
        "rs.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMpqaP7DcPND",
        "outputId": "249298b2-50ca-4704-f2df-014bbe0bacc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=5,\n",
              "                   estimator=CRF(algorithm='lbfgs',\n",
              "                                 all_possible_transitions=True,\n",
              "                                 max_iterations=100),\n",
              "                   n_iter=100, n_jobs=-1,\n",
              "                   param_distributions={'c1': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f9c2d2a5f70>,\n",
              "                                        'c2': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f9c2d0cb1c0>},\n",
              "                   scoring=make_scorer(flat_f1_score, average=weighted, labels=['B-NEG', 'I-NEG']),\n",
              "                   verbose=1)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('best params:', rs.best_params_)\n",
        "print('best CV score:', rs.best_score_)\n",
        "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMZ400qqeNea",
        "outputId": "d0c947c9-c732-4963-82c7-0bc37de67dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best params: {'c1': 0.02398642630933162, 'c2': 0.01151041318667512}\n",
            "best CV score: 0.9042006117562849\n",
            "model size: 0.09M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final testing set - We will use hypertuned model from above"
      ],
      "metadata": {
        "id": "p5RWWHAVcHVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test, y_test = process_data(test_data, True)\n",
        "y_test = filter(x_test,y_test)"
      ],
      "metadata": {
        "id": "mt9HqBZkhfhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crf = rs.best_estimator_\n",
        "y_pred = crf.predict(x_test)\n",
        "sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0]))\n",
        "print(metrics.flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fM4_NzlfUWOL",
        "outputId": "be8a6bfc-7598-424c-f3ad-8bc4109a3dfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-NEG      0.846     0.881     0.863       269\n",
            "       I-NEG      0.000     0.000     0.000         5\n",
            "\n",
            "   micro avg      0.846     0.865     0.856       274\n",
            "   macro avg      0.423     0.441     0.432       274\n",
            "weighted avg      0.831     0.865     0.848       274\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "def print_state_features(state_features):\n",
        "    for (attr, label), weight in state_features:\n",
        "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
        "\n",
        "print(\"Top positive:\")\n",
        "print_state_features(Counter(crf.state_features_).most_common(7))\n",
        "\n",
        "print(\"\\nTop negative:\")\n",
        "print_state_features(Counter(crf.state_features_).most_common()[-7:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PDhSQvd9qWE",
        "outputId": "20cd0ee7-e5fe-485f-b263-5b4ba9f009d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top positive:\n",
            "9.416724 B-NEG    lemma:nowhere\n",
            "6.933769 O        head:could\n",
            "6.384176 O        fine_pos:SCONJ\n",
            "6.341046 O        fine_pos:PRON\n",
            "6.164703 O        head:joke\n",
            "6.159647 O        head:may\n",
            "6.120044 O        head:does\n",
            "\n",
            "Top negative:\n",
            "-3.595956 O        lemma:never\n",
            "-3.758363 O        text:without\n",
            "-3.758363 O        lemma:without\n",
            "-4.268237 B-NEG    fine_pos:ADV\n",
            "-4.308088 O        lemma:nothing\n",
            "-4.660189 O        suffix\n",
            "-5.138984 B-NEG    fine_pos:NOUN\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "prvWX5nNORf4",
        "Dyc_QQ8XSKgt",
        "X7d-MsmDOeCX",
        "JcQPgzSOkyCZ",
        "RR5-pL5miPhX",
        "JEX7l7tQTeYY",
        "_qYZR8KdAIKq"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}