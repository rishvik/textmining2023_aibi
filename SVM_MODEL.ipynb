{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab48cc19-0391-4151-91b9-7168e2d36c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import argparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "982e57f7-af15-4a16-b9c2-2d8c765bd587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as mx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ddc7cd-3c34-4685-923d-4a8cbc05b695",
   "metadata": {},
   "source": [
    "Function for the extraction of differenct features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6ee8385-3780-4256-b751-fae59791f944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspire from from different documentations and stack over flow questions\n",
    "\n",
    "def wordnet_pos(pos):\n",
    "\n",
    "    if pos.startswith('J'):\n",
    "        return 'a'\n",
    "    elif pos.startswith('V'):\n",
    "        return 'v'\n",
    "    elif pos.startswith('N'):\n",
    "        return 'n'\n",
    "    elif pos.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "def lemma_extraction(tokens, pos_list):\n",
    "    lemmas = []\n",
    "    lem = WordNetLemmatizer()\n",
    "    for token, pos in zip(tokens, pos_list):\n",
    "        lemma = lem.lemmatize(token, wordnet_pos(pos))\n",
    "        lemmas.append(lemma)\n",
    "\n",
    "\n",
    "    return lemmas\n",
    "\n",
    "def pos_extraction(tokens):\n",
    "    pos_list = []\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    for token, pos_tag in tagged:\n",
    "        pos_list.append(pos_tag)   \n",
    "\n",
    "    return pos_list\n",
    "\n",
    "def previous_and_next_token_extraction(tokens):\n",
    "    position_index = 0\n",
    "\n",
    "    prev_tokens = []\n",
    "    next_tokens = []\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "\n",
    "        prev_index = (position_index - 1)\n",
    "        next_index = (position_index + 1)\n",
    "\n",
    "        #previous token\n",
    "        if prev_index < 0:\n",
    "            previous_token = \"None\"\n",
    "        else: \n",
    "            previous_token = tokens[prev_index]\n",
    "\n",
    "        prev_tokens.append(previous_token)\n",
    "\n",
    "        #next token\n",
    "        if next_index < len(tokens):\n",
    "            next_token = tokens[next_index]\n",
    "        else: \n",
    "            next_token = \"None\"\n",
    "\n",
    "        next_tokens.append(next_token)\n",
    "\n",
    "        #moving to next token in list \n",
    "        position_index += 1\n",
    "\n",
    "    return prev_tokens, next_tokens\n",
    "\n",
    "\n",
    "\n",
    "def neg(tokens, neg):\n",
    "    neg_word_list = []\n",
    "\n",
    "    for token in tokens:\n",
    "\n",
    "        # label 1 if token is in negative word list\n",
    "        if token in neg:\n",
    "            label = 1\n",
    "\n",
    "        # label 0 if token is not in negative word list\n",
    "        else:\n",
    "            label = 0\n",
    "        \n",
    "        neg_word_list.append(label)\n",
    "\n",
    "    return neg_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d7195-be8e-4b7e-9d0f-3da154fdd07f",
   "metadata": {},
   "source": [
    "Extracting features with the help of above functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcfadd75-d4b7-47b6-9b2b-c0e83d0cff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def file(input_file):\n",
    "    # open and red file and returns token, gold labels and chapters \n",
    "    \n",
    "    df = pd.read_csv(input_file, encoding='utf-8', delimiter = \"\\t\", quotechar = '|')\n",
    "    chapters = df.iloc[:, 0]\n",
    "    sent_id = df.iloc[:, 1]\n",
    "    token_position = df.iloc[:, 2]\n",
    "    tokens = df.iloc[:, 3]\n",
    "    gold = df.iloc[:, 4]\n",
    "    \n",
    "    return tokens, gold, chapters, \n",
    "\n",
    "def write_features(tokens,gold):\n",
    "    \n",
    "    negl= ['nor', 'neither', 'without', 'nobody', 'none', 'nothing', 'never', 'not', 'no', 'nowhere', 'non']\n",
    "    #not considering at this moment, will work on the smooth implimentation of this\n",
    "    # written the code for negetion words, fucntion can be seen above\n",
    "    # had few difificulties in final processing of output \n",
    "    \n",
    "    \n",
    "    # Defining header names\n",
    "    feature_names = [\"token\",\n",
    "                \"lemma\",\n",
    "                \"pos_tag\",\n",
    "                \"prev_token\",\n",
    "                \"next_token\",\n",
    "                    \"neg\"]\n",
    "\n",
    "   # using all above mentioned functions for the extraction of features, except fucntion neg  \n",
    "    pos_tag = pos_extraction(tokens)\n",
    "    lemma = lemma_extraction(tokens, pos_tag)\n",
    "    neg_word = neg(tokens,negl)\n",
    "    prev_next_token = previous_and_next_token_extraction(tokens)\n",
    "    prev_tokens, next_token = prev_next_token\n",
    "\n",
    "    features_dict = {'token': tokens, 'pos_tag': pos_tag,'lemma': lemma,'neg_word':neg_word ,'prev_token': prev_tokens,\n",
    "                     'next_token': next_token}\n",
    "    return features_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb2346-02d9-4b05-9e03-9a5b4f0eeaed",
   "metadata": {},
   "source": [
    "Creatinf featues list, svm model and prdictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccbee547-9119-43f0-a25a-ba5e440654fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data(input_file):\n",
    "    # main function to add features to dataset\n",
    "    data = []\n",
    "    tokens, gold, chapters= file(input_file) # from def file mentioned above\n",
    "    feature_dict = write_features(tokens,gold) # from above function\n",
    "    #features \n",
    "    token = feature_dict['token']\n",
    "    lemma = feature_dict['lemma']\n",
    "    pos_tag = feature_dict['pos_tag']\n",
    "    neg_word =  feature_dict['neg_word'],\n",
    "    prev_token= feature_dict['prev_token']\n",
    "    next_token= feature_dict['next_token']\n",
    "    golds = gold.tolist() #gold labels to list\n",
    "    token = token.tolist() #tokens to list \n",
    "    \n",
    "    #print(type(next_token)\n",
    "    #,type(token),type(lemma),type(pos_tag),type(neg_word),\n",
    "    # type(prev_token),type(next_token))\n",
    "    #print(type(list(neg_word)))\n",
    "    for token,lemma,pos_tag,prev_token,next_token in zip(token,lemma,pos_tag,\n",
    "                                                                         prev_token,next_token):\n",
    "        feature_dict = {\"Tokens\": token, 'lemmas':lemma,'pos_tags':pos_tag,\n",
    "                        'prev_token':prev_token,'next_token':next_token}\n",
    "\n",
    "        #feature_dict = {\"1\": token, '2':lemma,'3':pos_tag,'4':neg_word,'5':prev_token,'6':next_token, '7': gold}\n",
    "        data.append(feature_dict)\n",
    "        #print(data)\n",
    "    return data, golds\n",
    "\n",
    "def create_classifier(train_features, train_targets: list):\n",
    "    #creating SVM model \n",
    "    \n",
    "    model = LinearSVC()\n",
    "    vec = DictVectorizer()\n",
    "    featuresv = vec.fit_transform(training_features)\n",
    "    train_targets = np.array(train_targets)\n",
    "    model.fit(featuresv, train_targets)\n",
    "\n",
    "    return model, vec\n",
    "\n",
    "\n",
    "\n",
    "def predicted(testfile, model,vec):\n",
    "    #prediction of SVM on test and dev set\n",
    "    features, goldlabels = data(testfile)\n",
    "    test_features_vectorized = vec.transform(features)\n",
    "    predictions = model.predict(test_features_vectorized)\n",
    "\n",
    "    return predictions, goldlabels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08861e67-67dc-4c02-8458-c32b88ee88f0",
   "metadata": {},
   "source": [
    "Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "767096de-7600-4021-9da0-273a1a08e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = 'data/no_dev.txt'\n",
    "dev_file = 'data/dev.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd252487-d291-4223-9b88-2d815ccaf702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging two test set, circle and cardboard into one \n",
    "test_card = pd.read_csv('data/test_1.txt', sep=\"\\t\", header = None)#cardboard test\n",
    "test_circ = pd.read_csv('data/test_2.txt', sep=\"\\t\", header = None)#circle test\n",
    "\n",
    "frames = [test_card, test_circ]\n",
    "test_data = pd.concat(frames)\n",
    "#test_data.rename(columns={1: 'phrase_id', 0: 'chapter_id', 2:'word_id', 3:'word', 4:'label'}, inplace=True)\n",
    "test_data.to_csv('merge_test_.txt', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f328ab93-c097-4f2a-8998-3815c7417f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merge = \"merge_test_.txt\" # final test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72cda19f-17d7-40ea-bb49-7cffc074552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating TRAINING FEATURES \n",
    "training_features, gold_labels= data(training_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20768578-a8cd-4564-9331-9f1ee7990f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    " #creating model and vectorzor \n",
    "model, vec= create_classifier(training_features, gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fe79518-94c7-4f3d-b378-9152a288ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# develoment predictions \n",
    "dev, gold = predicted(dev_file, model, vec)\n",
    "# test predictions \n",
    "test1, gold1 = predicted(test_merge, model, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52e3dbd2-3721-4b32-a9a3-f0c6f0f7734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding predictions to the df created by mearging the files. \n",
    "test_data['SVM__predictions'] = test1\n",
    "test_data.rename(columns={1: 'sent_id', 0: 'chapter', 2:'word_id', 3:'token', 4:'gold'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd84a7f1-a9d8-4463-a1df-2c2cd2683ad6",
   "metadata": {},
   "source": [
    "Classification report and P,R,f-1scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fcfcc33-68b8-4a43-9b4d-9ce7de2f5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(predictions, goldlabels):\n",
    "    # based on example from https://datatofish.com/confusion-matrix-python/\n",
    "    data = {'Gold': goldlabels, 'Predicted': predictions}\n",
    "    df = pd.DataFrame(data, columns=['Gold', 'Predicted'])\n",
    "\n",
    "    confusion_matrix = pd.crosstab(df['Gold'], df['Predicted'], rownames=['Gold'], colnames=['Predicted'])\n",
    "    print(confusion_matrix)\n",
    "    report = classification_report(goldlabels,predictions,digits = 3)\n",
    "    print('METRICS: ')\n",
    "    print()\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9e232f-957e-4ac4-bc53-b76392cbed3c",
   "metadata": {},
   "source": [
    "Development se classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b0e459f-c9d7-425f-a1c0-51df88e6dcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  B-NEG  I-NEG      O\n",
      "Gold                          \n",
      "B-NEG        149      0     27\n",
      "I-NEG          0      2      1\n",
      "O              6      0  13381\n",
      "METRICS: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-NEG      0.961     0.847     0.900       176\n",
      "       I-NEG      1.000     0.667     0.800         3\n",
      "           O      0.998     1.000     0.999     13387\n",
      "\n",
      "    accuracy                          0.997     13566\n",
      "   macro avg      0.986     0.838     0.900     13566\n",
      "weighted avg      0.997     0.997     0.997     13566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cf_dev = print_confusion_matrix(dev,gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51685a1a-6c99-4148-9452-60e2f3d2d1b9",
   "metadata": {},
   "source": [
    "Test set classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bbfb66c-8587-4d69-8f1f-81e802e98f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  B-NEG      O\n",
      "Gold                   \n",
      "B-NEG        247     22\n",
      "I-NEG          0      5\n",
      "O             13  18929\n",
      "METRICS: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-NEG      0.950     0.918     0.934       269\n",
      "       I-NEG      0.000     0.000     0.000         5\n",
      "           O      0.999     0.999     0.999     18942\n",
      "\n",
      "    accuracy                          0.998     19216\n",
      "   macro avg      0.650     0.639     0.644     19216\n",
      "weighted avg      0.998     0.998     0.998     19216\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishvikchandel/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/rishvikchandel/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/rishvikchandel/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cf_test = print_confusion_matrix(test1,gold1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fdfacc-a065-4e62-9f84-1530f66370a4",
   "metadata": {},
   "source": [
    "Error analysis on test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f2e2a3f-6feb-44a4-9831-e74fb2a39386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_detect(tokens, gold, chapters, sent_id, pred):\n",
    "    #write all the errors in a tsv file \n",
    "    #take inouts from test set and give error examples \n",
    "    error_dict = []\n",
    "    for token, gol, chap,sent, pre in zip(tokens, gold,chapters, sent_id, pred):\n",
    "        if gol != pre:\n",
    "            error_sent = {\"chap\": chap, \"sent_id\": sent, \"Token\": token, \"Gold\": gol, \"Prediction\": pre}\n",
    "            error_dict.append(error_sent)\n",
    "            df= pd.DataFrame(error_dict)\n",
    "            df.to_csv('error_test.tsv', sep='\\t', header = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d47db48c-5196-464c-91b6-9c73cb16ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_detect(test_data['token'],test_data['gold'],test_data['chapter'],test_data['sent_id'],test_data['SVM__predictions'],)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae1ca7-dc5a-4da6-a8e0-c2da8b2e301b",
   "metadata": {},
   "source": [
    "Feature ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "312bbb82-ad5b-474a-8306-90a1e6cb624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature ablation using svm's svc \n",
    "\n",
    "def score_model(features_train, gold1):\n",
    "    clf = SVC(gamma='scale', kernel='rbf') #dvc model\n",
    "    vec = DictVectorizer()\n",
    "    featuresv = vec.fit_transform(features_train)\n",
    "    #featurest = vec.fit_transform(features_test)\n",
    "    train_targets = np.array(gold1)\n",
    "    clf = clf.fit(featuresv, train_targets)\n",
    "   # pred = clf.predict(featurest)\n",
    "   # return mx.accuracy_score(gold2, pred)\n",
    "    return clf, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a14b65ec-032e-43ba-a6d5-0dbe7a8249b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1, vec1= score_model(training_features, gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "24140106-07a2-481b-b695-4abab8e0584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(merge, model,vec): \n",
    "    #prediction on full features \n",
    "    test, gold = data(merge)\n",
    "    test_ = vec.transform(test)\n",
    "    pred = model.predict(test_)\n",
    "    #print(test_)\n",
    "    return  mx.accuracy_score(gold, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "36530f22-4480-4d3c-a271-cb6daa91059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = prediction(test_merge, model1, vec1) # acc on all features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "ca5eb1f4-c918-4bfa-ae31-1647bd8d29da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "Tokens 0.9907889258950874 [0.00603663613655292]\n",
      "lemmas 0.9907889258950874 [0.00603663613655292]\n",
      "pos_tags 0.9976582014987511 [-0.0008326394671107629]\n",
      "prev_token 0.9977622814321399 [-0.0009367194004995527]\n",
      "next_token 0.9971378018318068 [-0.00031223980016648056]\n"
     ]
    }
   ],
   "source": [
    "#removing one feature at a time to check accuracy\n",
    "print(test1[1])\n",
    "columns= ['Tokens','lemmas','pos_tags',\t'prev_token',\t'next_token'] # all features \n",
    "for feature in columns:\n",
    "    feature_importances = []\n",
    "    test, gold = data(test_merge)\n",
    "    #print(feature)\n",
    "    \n",
    "    for items in test:\n",
    "       # print(items)\n",
    "        if feature in  items:\n",
    "            del items[feature]\n",
    "            #X_test_ablated = X_test.drop(feature, axis=1)\n",
    "    #print(test[1])\n",
    "    test_ = vec.transform(test)\n",
    "    y_pred_ablated = model.predict(test_)\n",
    "    feature_importance1 = mx.accuracy_score(gold, y_pred_ablated) #accuracy after droping that feature \n",
    "    #print(feature_importance)\n",
    "    # Calculate the difference in f1 score compared to the baseline\n",
    "    feature_importance = mx1 - feature_importance1\n",
    "    feature_importances.append(feature_importance)\n",
    "    print(feature,feature_importance1,  feature_importances)\n",
    "#print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53571c18-7740-4201-a68e-9fe5387ac4c4",
   "metadata": {},
   "source": [
    "above mentioned is the feature removed, accuracy after removing that feature and acctual accuracy - accuracy after removing thath feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73c0753-3b36-4450-a8ce-4564b49990d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
